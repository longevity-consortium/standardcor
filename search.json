[{"path":"https://longevity-consortium.github.io/standardcor/articles/Getting-Started.html","id":"null-models-of-spurious-correlations","dir":"Articles","previous_headings":"","what":"Null models of spurious correlations","title":"Getting-Started","text":"make assumption relationship among features interest; , bulk pairwise correlations features data likely spurious, either due correlations produced measurement process due ordinary variation data. analysis begin trying characterize distribution spurious correlations null model. explained paper, strong argument due mathematical formula defining Pearson (therefore Spearman) correlations, spurious correlations produce ${\\rm Beta}$ distribution range possible correlation values ([âˆ’1,1][-1,1]). Since typical range ${\\rm Beta}$ distribution [0,1][0,1], can represent distribution range ${\\rm Beta}$ distribution $r \\thicksim 2 {\\rm Beta}(v,w) - 1$, vv ww shape parameters ${\\rm Beta}$ distribution. intuition behind model vv ww expected number yesses nos among total v+wv+w votes, respectively, yesses votes pure positive correlation (11) nos votes pure negative correlation (âˆ’1-1). start simulating toy dataset containing spurious correlation, data 200200 samples measured 500500 features, data value sampled independently standard Normal distribution. One consider data matrix M={msg}={xg[s]}M = \\{m_{sg}\\} = \\{x_g[s]\\} contain normalized transcript abundances transcriptomics experiment, measured abundance kg[s]k_g[s] gene gg sample ss following gene-specific Lognormal distribution, log transformation scale normalization, xg[s]=logkg[s]âˆ’logkgÂ¯x_g[s] = \\log{k_g[s]} - \\overline{\\log{k_g}}. signal detect data, noise; later make data complex illustrate working package.","code":"Samples <- 200 Genes <- 500 M <- Matrix(rnorm(Samples*Genes),nrow=Samples,ncol=Genes,sparse=TRUE) colnames(M) <- paste(\"G\",c(1:Genes)) rownames(M) <- paste(\"S\",c(1:Samples)) dim(M) #> [1] 200 500"},{"path":"https://longevity-consortium.github.io/standardcor/articles/Getting-Started.html","id":"step-1-computing-correlations","dir":"Articles","previous_headings":"Null models of spurious correlations","what":"Step 1: Computing Correlations","title":"Getting-Started","text":"provide function ${\\rm standardcor::SparseSpearmanCor2}()$ compute correlation matrix primarily efficiency, data matrices can quite large; number transcripts can easily 20,00020,000 . algorithm ${\\rm SparseSpearmanCor2()}$, due Saket Choudhary Mumbai, India (https://github.com/saketkc/blog/blob/main/2022-03-10/SparseSpearmanCorrelation2.ipynb), highly efficient compared methods computing Spearman correlations. Despite sparse implementation, find least efficient common methods even dense data matrices. Use function entirely optional, rest ${\\rm standardcor}$ package depend correlations computed. Although opinion â€™omics data collected biological samples tends enough meaningful outliers rank correlation preferable, theory package based perfectly appropriate Pearson correlations well. now data matrix MM corresponding correlation matrix ZZ.","code":"Z <- SparseSpearmanCor2(M) dim(Z) #> [1] 500 500 colnames(Z) <- colnames(M) rownames(Z) <- colnames(Z)"},{"path":"https://longevity-consortium.github.io/standardcor/articles/Getting-Started.html","id":"step-2-estimating-the-distribution-of-spurious-correlations","dir":"Articles","previous_headings":"Null models of spurious correlations","what":"Step 2: Estimating the distribution of spurious correlations","title":"Getting-Started","text":"provide function ${\\rm standardcor::estimateShape}()$ purpose. assumption correlations pairs variables spurious, heuristic function estimates parameters Beta distribution $2 {\\rm Beta}(v,w) - 1$ fits bulk correlations; , attempts fit distribution best vicinity mode. similar making robust estimate, intended work presence outliers; however real data whether particular value considered outlier rarely known, function therefore attempts solve ill-defined problem. use subsequent steps, important shape parameters vv ww provide excellent fit correlation values researcher considers spurious, whether function used find . permit behavior function adjusted evaluated, provide parameters ${\\rm left}$ ${\\rm right}$, shift fitted mode indicated direction. rough guidelines adjustments, mode $2 {\\rm Beta}(v,w) - 1$ $2 {v-1 \\v+w-2}-1$, given mode, sum v+wv+w governs width distribution proportion $1 \\\\sqrt{v+w}$, .e taller narrower sum increases. Increasing decreasing ${\\rm right}$ increases decreases vv, likewise ${\\rm left}$ adjusts ww. effect adjustments can assessed using parameter ${\\rm plot = TRUE}$. One standard way estimate parameters ${\\rm Beta}$ distribution, computing mean variance, shown comparison. Since dataset simulated outliers, two methods produce similar estimates. 12251225 unique, non-identical correlations MM, theoretically parameters distribution v=w=200/2âˆ’1=99v = w = 200/2 - 1 = 99; estimates reasonably close value, seen fit curves data.  second example modify test data, mixing Bias vector (copied required number times) first 20%20\\% features. biased features therefore correlated weight given Bias vector, case r=0.5r = 0.5, addition 50%50\\% original spurious correlation. pairwise correlations among features, roughly 20%Ã—20%=4%20\\% \\times 20\\% = 4\\% unique, non-identical pairs, simulated â€œsignalâ€ relative remaining roughly 96%96\\% purely spurious correlations. example shows Method Moments used estimate null model presence outliers.  third example, show difference using null model directly distance versus using null model penalty top typical Euclidean distance corresponding corrolation coefficient. betaDistance() function penalizes correlation coefficients likely null model, doesnâ€™t penalize correlation coefficients larger predicted null model. Without including Euclidean distance, essentially distance highly correlated pairs; including Euclidean distance, highly correlated pairs can still separated degree correlation.","code":"# row(Z) < col(Z): the correlation matrix is square and symmetric, with diagonal 1. # This condition limits the set of correlations that are modeled to the upper right triangle, which # constitutes the nonredundant, non-self correlations among the variables. beta.shape <- estimateShape(Z[row(Z) < col(Z)], plot=TRUE, ylim=c(0,7), fine=60,                             main=c(paste(Samples,\"samples,\",Genes,\"features\"),                                    \"all correlations purely spurious\")) title(paste(round(beta.shape,2),collapse=\", \"),line=-1) #text(x=-0.5,y=2.0,labels=paste(\"D/2 - 1 = \",round(Genes/2-1, 2)),adj = 1/2)  R <- c(-100:100)/100 ; R <-ifelse(-1 < R, ifelse(R < 1, R, 1), -1) # Ensure R in [-1,1]  x     <- (1+Z[row(Z) < col(Z)])/2 x.mu  <- mean(x) x.var <- var(x) z.mm <- x.mu*(1-x.mu) / x.var - 1 v.mm <- x.mu * z.mm w.mm <- (1-x.mu) * z.mm  lines(R,dbeta((1+R)/2, v.mm, w.mm)/2, lwd=3, col='forestgreen') text(x=0.5,y=c(2.5,2),      labels=c(\"Method of Moments\",paste(\"Beta(\",round(v.mm, 2),\",\",round(w.mm,2),\")\",sep='')),      col='forestgreen',adj = 1/2) corr <- 0.5 pct <- 20 # Adjusted Step 1 M2 <- M Altered <- round(Genes*pct/100) # The first Altered variables will be intercorrelated Bias <- matrix(rep(rnorm(Samples),Altered),nrow=Samples,ncol=Altered)  M2[,c(1:Altered)] <- (1-corr) * M2[,c(1:Altered)] + corr*Bias Z2 <- SparseSpearmanCor2(M2)  # Repeat Step 2 beta.shape <- estimateShape(Z2[row(Z2) < col(Z2)], plot=TRUE, ylim=c(0,7), fine=60,                             main=c(paste(Samples,\"samples,\",Genes,\"features\"),                                    paste(Altered,\"correlated features, r =\",corr))) title(paste(round(beta.shape,2),collapse=\", \"),line=-1) #text(x=-0.5,y=2.0,labels=paste(\"D/2 - 1 = \",round(Genes/2-1, 2)),adj = 1/2)  R <- c(-100:100)/100 ; R <-ifelse(-1 < R, ifelse(R < 1, R, 1), -1) # Ensure R in [-1,1]  x     <- (1+Z2[row(Z2) < col(Z2)])/2 x.mu  <- mean(x) x.var <- var(x) z.mm <- x.mu*(1-x.mu) / x.var - 1 v.mm <- x.mu * z.mm w.mm <- (1-x.mu) * z.mm  lines(R,dbeta((1+R)/2, v.mm, w.mm)/2, lwd=3, col='forestgreen') text(x=0.5,y=c(5,4.25),      labels=c(\"Method of Moments\",paste(\"Beta(\",round(v.mm, 2),\",\",round(w.mm,2),\")\",sep='')),      col='forestgreen',adj = 1/2) v <- mean(beta.shape) # Expect v = (D-1)/2 = 24.5 v.name <- paste(\"Beta(\",paste(round(c(v,v),2),collapse=\", \"),\")\",sep='') plot(R,betaDistance(R, v, v, mix=0), type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Distance Penalty\",     main = paste(v.name,\"penalty\")) abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey') plot(R,betaDistance(R, v, v, mix=1),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Relative Distance\",     main = c(\"Euclidean distance squared mixed with\",              paste(v.name, \"penalty\"))) abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')"},{"path":"https://longevity-consortium.github.io/standardcor/articles/Getting-Started.html","id":"multi-omic-data-example","dir":"Articles","previous_headings":"","what":"Multi-omic data example","title":"Getting-Started","text":"Proteomics correlation matrix 284 x 284 0 na values -0.641119940479762 <= cor <= 1","code":"data('synth_LC') names(synth_LC) <- c('Metadata','Metabolomics','Proteomics') Z.met <- SparseSpearmanCor2(as.matrix(synth_LC[['Metabolomics']])) dim(Z.met) #> [1] 2647 2647 length(which(is.na(Z.met))) #> [1] 0 range(Z.met,na.rm=TRUE) #> [1] -0.8189155  1.0000000 shape.Met <- estimateShape(Z.met[row(Z.met) < col(Z.met)], plot=TRUE) title(paste('Metabolite correlations',paste(round(shape.Met,2),collapse=\", \")),line=0.5) Z.prot <- SparseSpearmanCor2(as.matrix(synth_LC[['Proteomics']])) shape.prot <- estimateShape(Z.prot[row(Z.prot) < col(Z.prot)], plot=TRUE) title(paste('Protein correlations',paste(round(shape.prot,2),collapse=\", \")),line=0.5) N <- dim(synth_LC[['Metabolomics']])[2] Met <- as.matrix(synth_LC[['Metabolomics']][,2:N]) rownames(Met) <- synth_LC[['Metabolomics']]$subjectID  N <- dim(synth_LC[['Proteomics']])[2] Prot <- as.matrix(synth_LC[['Proteomics']][,2:N]) rownames(Prot) <- synth_LC[['Proteomics']]$subjectID par(mar = c(2, 2, 4, 2), mfrow=c(2,2)) L <- multiOmicModel(list(met = Met, prot = Prot), plot=TRUE, annotate=TRUE, fine=30)  v.std <- 32 Z <- standardizeFromModel(L$modelL, L$analyteL, v.std) adjTable1 <- nullModelAdjacencyTable(Z[row(Z) < col(Z)], v.std, scale=1) adjTable2 <- nullModelAdjacencyTable(Z[row(Z) < col(Z)], v.std) adjTable3 <- nullModelAdjacencyTable(Z[row(Z) < col(Z)], v.std, scale=3)  shape <- estimateShape(Z[row(Z) < col(Z)], plot=TRUE, main = \"Standardized\", fine=30) title(paste(\"Beta(\",v.std,\",\",v.std,\")\",sep=''), line = 0.5) Bs <- c(-100:100)/100 lines(Bs,interpolatedAdjacency(Bs,adjTable1),lwd=3,col='orangered') lines(Bs,interpolatedAdjacency(Bs,adjTable2),lwd=3,col='forestgreen') lines(Bs,interpolatedAdjacency(Bs,adjTable3),lwd=3,col='dodgerblue')"},{"path":"https://longevity-consortium.github.io/standardcor/articles/Getting-Started.html","id":"beta-distance-function","dir":"Articles","previous_headings":"","what":"Beta distance function","title":"Getting-Started","text":"","code":"R <- c(-100:100)/100 ; R <- ifelse(R < -1, -1, ifelse(1 < R, 1, R))  par(mar = c(2, 2, 4, 2), mfrow=c(2,2))  plot(R,betaDistance(R,1,1,mix=0,unsigned=FALSE),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 1, w = 1), signed, unmixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,betaDistance(R, 3, 6, mix=0, unsigned=FALSE),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 3, w = 6), signed, unmixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,betaDistance(R, 6, 3, mix=0,unsigned=FALSE),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 6, w = 3), signed, unmixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,betaDistance(R, 30, 30,mix=0,unsigned=FALSE),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 30, w = 30), signed, unmixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey') R <- c(-100:100)/100 ; R <- ifelse(R < -1, -1, ifelse(1 < R, 1, R))  par(mar = c(2, 2, 4, 2), mfrow=c(2,2))  plot(R,betaDistance(R,1,1,mix=1,unsigned=FALSE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 1, w = 1), signed, mixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,betaDistance(R, 3, 6, mix=1, unsigned=FALSE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 3, w = 6), signed, mixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,betaDistance(R, 6, 3, mix=1,unsigned=FALSE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 6, w = 3), signed, mixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,betaDistance(R, 30, 30,mix=1,unsigned=FALSE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 30, w = 30), signed, mixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey') R <- c(-100:100)/100 ; R <- ifelse(R < -1, -1, ifelse(1 < R, 1, R))  par(mar = c(2, 2, 4, 2), mfrow=c(2,2))  plot(R,betaDistance(R,1,1,mix=0),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 1, w = 1), unsigned, unmixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,betaDistance(R, 3, 6, mix=0),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 3, w = 6), unsigned, unmixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,betaDistance(R, 6, 3, mix=0),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 6, w = 3), unsigned, unmixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,betaDistance(R, 30, 30,mix=0),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 30, w = 30), unsigned, unmixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey') R <- c(-100:100)/100 ; R <- ifelse(R < -1, -1, ifelse(1 < R, 1, R))  par(mar = c(2, 2, 4, 2), mfrow=c(2,2))  plot(R,betaDistance(R,1,1),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 1, w = 1), unsigned, mixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,betaDistance(R, 3, 6),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 3, w = 6), unsigned, mixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,betaDistance(R, 6, 3),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 6, w = 3), unsigned, mixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,betaDistance(R, 30, 30),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Beta Distance\",     main = \"Beta(v = 30, w = 30), unsigned, mixed\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')"},{"path":"https://longevity-consortium.github.io/standardcor/articles/Getting-Started.html","id":"power-distance-function","dir":"Articles","previous_headings":"","what":"Power distance function","title":"Getting-Started","text":"","code":"R <- c(-100:100)/100 ; R <- ifelse(R < -1, -1, ifelse(1 < R, 1, R))  par(mar = c(2, 2, 4, 2), mfrow=c(2,2))  plot(R,powerDistance(R,k=3,unsigned=FALSE),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Power Distance\",     main = \"k = 3, unsigned\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,powerDistance(R,k=6, unsigned=FALSE),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Power Distance\",     main = \"k = 6, unsigned\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,powerDistance(R,k=12, unsigned=FALSE),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Power Distance\",     main = \"k = 12, unsigned\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,powerDistance(R,k=0.5, unsigned=FALSE),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Power Distance\",     main = \"k = 0.5, unsigned\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey') R <- c(-100:100)/100 ; R <- ifelse(R < -1, -1, ifelse(1 < R, 1, R))  par(mar = c(2, 2, 4, 2), mfrow=c(2,2))  plot(R,powerDistance(R,k=3),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Power Distance\",     main = \"k = 3, unsigned\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,powerDistance(R,k=6),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Power Distance\",     main = \"k = 6, unsigned\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,powerDistance(R,k=12),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Power Distance\",     main = \"k = 12, unsigned\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')  plot(R,powerDistance(R,k=0.5),type='l',lwd=3, ylim=c(0,1),      xlab=\"Correlation coefficient\", ylab=\"Power Distance\",     main = \"k = 0.5, unsigned\") abline(h=c(-1:1),col='grey') ; abline(v=c(0,1),col='grey')"},{"path":"https://longevity-consortium.github.io/standardcor/articles/Getting-Started.html","id":"sigmoid-distance-function","dir":"Articles","previous_headings":"","what":"Sigmoid distance function","title":"Getting-Started","text":"","code":"R <- c(-100:100)/100 ; R <- ifelse(R < -1, -1, ifelse(1 < R, 1, R))  par(mar = c(2, 2, 4, 2), mfrow=c(2,2))  plot(R,sigmoidDistance(R,alpha=5,tau0 = 0,unsigned=FALSE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Sigmoid Distance\",     main = \"sigmoid(5, 0), unsigned\",ylim=c(0,1)) abline(v=c(-1:1),col='grey') ; abline(h=c(0,1),col='grey') abline(h=0.5, lty=2, col='grey')  plot(R,sigmoidDistance(R,alpha=5,tau0 = 1,unsigned=FALSE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Sigmoid Distance\",     main = \"sigmoid(5, 1), unsigned\",ylim=c(0,1)) abline(v=c(-1:1),col='grey') ; abline(h=c(0,1),col='grey') abline(h=0.5, lty=2, col='grey')  plot(R,sigmoidDistance(R,alpha=25,tau0 = -0.5,unsigned=FALSE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Sigmoid Distance\",     main = \"sigmoid(25, -1/2), unsigned\",ylim=c(0,1)) abline(v=c(-1:1),col='grey') ; abline(h=c(0,1),col='grey') abline(h=0.5, lty=2, col='grey')  plot(R,sigmoidDistance(R,alpha=5,tau0 = 1,unsigned=FALSE, stretch=TRUE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Sigmoid Distance\",     main = \"sigmoid(5, 1), unsigned, stretched\",ylim=c(0,1)) abline(v=c(-1:1),col='grey') ; abline(h=c(0,1),col='grey') abline(h=0.5, lty=2, col='grey') R <- c(-100:100)/100 ; R <- ifelse(R < -1, -1, ifelse(1 < R, 1, R))  par(mar = c(2, 2, 4, 2), mfrow=c(2,2))  plot(R,sigmoidDistance(R,alpha=4,tau0 = 1/4,unsigned=FALSE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Sigmoid Distance\",     main = \"sigmoid(4, 1/4), unsigned\",ylim=c(0,1)) abline(v=c(-1:1),col='grey') ; abline(h=c(0,1),col='grey') abline(h=0.5, lty=2, col='grey')  plot(R,sigmoidDistance(R,alpha=10,tau0 = -1/4,unsigned=FALSE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Sigmoid Distance\",     main = \"sigmoid(10, -1/4), unsigned\",ylim=c(0,1)) abline(v=c(-1:1),col='grey') ; abline(h=c(0,1),col='grey') abline(h=0.5, lty=2, col='grey')  plot(R,sigmoidDistance(R,alpha=20,tau0 = 1/4,unsigned=FALSE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Sigmoid Distance\",     main = \"sigmoid(20, 1/4), unsigned\",ylim=c(0,1)) abline(v=c(-1:1),col='grey') ; abline(h=c(0,1),col='grey') abline(h=0.5, lty=2, col='grey')  plot(R,sigmoidDistance(R,alpha=100,tau0 = -1/4,unsigned=FALSE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Sigmoid Distance\",     main = \"sigmoid(100, -1/4), unsigned\",ylim=c(0,1)) abline(v=c(-1:1),col='grey') ; abline(h=c(0,1),col='grey') abline(h=0.5, lty=2, col='grey') R <- c(-100:100)/100 ; R <- ifelse(R < -1, -1, ifelse(1 < R, 1, R))  par(mar = c(2, 2, 4, 2), mfrow=c(2,2))  plot(R,sigmoidDistance(R,alpha=1,tau0 = 1/2,unsigned=TRUE,stretch=TRUE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Sigmoid Distance\",     main = \"sigmoid(1, 1/2), unsigned, stretched\",ylim=c(0,1)) abline(v=c(-1:1),col='grey') ; abline(h=c(0,1),col='grey') abline(h=0.5, lty=2, col='grey')  plot(R,sigmoidDistance(R,alpha=8,tau0 = 1,unsigned=TRUE,stretch=TRUE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Sigmoid Distance\",     main = \"sigmoid(8, 1), unsigned, stretched\",ylim=c(0,1)) abline(v=c(-1:1),col='grey') ; abline(h=c(0,1),col='grey') abline(h=0.5, lty=2, col='grey')  plot(R,sigmoidDistance(R,alpha=4,tau0 = 0,unsigned=TRUE,stretch=TRUE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Sigmoid Distance\",     main = \"sigmoid(4, 0), unsigned, stretched\",ylim=c(0,1)) abline(v=c(-1:1),col='grey') ; abline(h=c(0,1),col='grey') abline(h=0.5, lty=2, col='grey')  plot(R,sigmoidDistance(R,alpha=100,tau0 = 1/10,unsigned=TRUE,stretch=TRUE),type='l',lwd=3,      xlab=\"Correlation coefficient\", ylab=\"Sigmoid Distance\",     main = \"sigmoid(100, 1/10), unsigned, stretched\",ylim=c(0,1)) abline(v=c(-1:1),col='grey') ; abline(h=c(0,1),col='grey') abline(h=0.5, lty=2, col='grey')"},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"wgcna-integration-of-multi-omics","dir":"Articles","previous_headings":"","what":"WGCNA, Integration of multi-omics","title":"standardcor-WGCNA","text":"notebook covers correlation standardization technique (information : https://github.com/PriceLab/standardcor) combine omics downstream WGCNA analysis. combine omics, start correlations separately omics. omics tends different distribution correlations; example, randomly selected analytes proteomics assay tends positively correlated (also true transcriptomics), randomly selected metabolites typically closer uncorrelated. issue computing adjacencies single â€™omics, transformation correlations adjacencies (WGCNA) fit separate distributions separately. combine correlations across different â€™omics, analytes measured different â€™omics experiments, WGCNA attempt find one power ð‘˜ fit , result different distributions adjacency analyte pairs different source experiments. One particular source differences distribution sample size. compute correlations metabolites 1,000 individuals, expect much accurate correlation values compute 10; sampling variation alone result sqrt(1000/10) = 10-fold difference variance! therefore make smooth model distribution correlations type, transform correlation values single, shared smooth model. resulting values longer interpretable correlations, standardize significance correlations different sources onto single, shared significance scale. WGCNA fits values scale, applying significance standards correlations, regardless original source.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"data-and-preprocessing","dir":"Articles","previous_headings":"WGCNA, Integration of multi-omics","what":"Data and preprocessing","title":"standardcor-WGCNA","text":"required input files : Phenotype Table - containing outcome inPrimary Merest MetabolitPrimary e Table - metabolite abunda Lipid Table - Lipid abundance values Biogenic Amine Table - Biogenic Amine Tablence values Protein Table - protein abundance values Data analysis synthesized Longevity Consortium generated proteomic metabolomic data. used Gaussian Mixture Model (GMM) create 1000 synthetic participants omic type, 500 cases 500 controls, followed addition synthetic case/control signal. signal generated adding 0.1 set proteins metabolites cases. original Longevity Consortium datasets filtered high missingness (>20%), imputed using random-forest imputation log normalized prior running GMM.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"load-data","dir":"Articles","previous_headings":"WGCNA, Integration of multi-omics","what":"Load data","title":"standardcor-WGCNA","text":"","code":"#Phenotypes pheno <- read_delim(\"../data/WGCNA/case_synth.tsv\", show_col_types = FALSE)  ### Load proteins prots <- read_delim(\"../data/WGCNA/proteins_synth.tsv\", show_col_types = FALSE)  ### Metabolites lipids_df <- read_delim(\"../data/WGCNA/Lipids_synth.tsv\", show_col_types = FALSE) amines_df  <- read_delim(\"../data/WGCNA/BA_synth.tsv\", show_col_types = FALSE) primary_df  <- read_delim(\"../data/WGCNA/Primary_synth.tsv\", show_col_types = FALSE)   ### Metabolite features features <- read_delim(\"../data/WGCNA/Met_Features.tsv\", show_col_types = FALSE) ## Merge together in_df <- merge(primary_df, lipids_df, by=\"subjectID\") in_df <- merge(in_df, amines_df, by=\"subjectID\") in_df <- merge(in_df, prots, by=\"subjectID\")  # Check dimensions of each dataframe dim(primary_df) #> [1] 500 150 dim(lipids_df) #> [1] 500 713 dim(amines_df) #> [1] 500 322 dim(prots) #> [1] 500 284 dim(in_df) #> [1]  500 1466 dim(pheno) #> [1] 500   2 # Drop id column and get features num.analytes <- setdiff(unique(c(colnames(primary_df),colnames(lipids_df),colnames(amines_df),colnames(prots))),'subjectID') num_df <- in_df[,colnames(in_df) %in% num.analytes] num_df <- as.matrix(num_df) rownames(num_df) <- in_df$subjectID ## Filter samples and features based on WGCNA NA criteria (50%) gsg = goodSamplesGenes(num_df, verbose = 5); #>  Flagging genes and samples with too many missing values... #>   ..step 1 gsg$allOK #> [1] TRUE if (!gsg$allOK) {   # Optionally, print the gene and sample names that were removed:   if (sum(!gsg$goodGenes)>0)      printFlush(paste(\"Removing genes:\", paste(names(num_df)[!gsg$goodGenes], collapse = \", \")));   if (sum(!gsg$goodSamples)>0)      printFlush(paste(\"Removing samples:\", paste(rownames(num_df)[!gsg$goodSamples], collapse = \", \")));   # Remove the offending genes and samples from the data:   num_df = num_df[gsg$goodSamples, gsg$goodGenes] }  dim(num_df) #> [1]  500 1465 # Get the names of remaining analytes overall (all.analytes) by category cat.prots <- intersect(colnames(prots),colnames(num_df)) cat.primary  <- intersect(colnames(primary_df),colnames(num_df)) cat.lipid <- intersect(colnames(lipids_df),colnames(num_df)) cat.amines <- intersect(colnames(amines_df),colnames(num_df))  all.analytes <- c(cat.prots,cat.primary,cat.lipid,cat.amines) print(paste(length(cat.prots),length(cat.lipid),length(cat.primary),length(cat.amines),length(all.analytes))) #> [1] \"283 712 149 321 1465\"  # We will construct a correlation matrix Z in parts corresponding to each category of analyte pairs. n.analytes <- length(all.analytes) # To compute correlations, features must be numeric. We will use Spearman all_df <- num_df[,all.analytes]"},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"generate-correlations","dir":"Articles","previous_headings":"WGCNA, Integration of multi-omics","what":"Generate Correlations","title":"standardcor-WGCNA","text":"use implementation sparse spearman correlation saves memory time. helps prevent notebook crashing! use Spearman rank correlationsince affected scalar multiples log-transformation. information technique can found : https://github.com/saketkc/blog/blob/main/2022-03-10/SparseSpearmanCorrelation2.ipynbs","code":"s.prots <- as(all_df[,cat.prots], \"sparseMatrix\") s.primary <- as(all_df[,cat.primary], \"sparseMatrix\") s.lipid <- as(all_df[,cat.lipid], \"sparseMatrix\") s.amines <- as(all_df[,cat.amines], \"sparseMatrix\") # Within-category correlations Z.pp <- SparseSpearmanCor2(s.prots) Z.mm <- SparseSpearmanCor2(s.primary) Z.ll <- SparseSpearmanCor2(s.lipid) Z.aa <- SparseSpearmanCor2(s.amines) # Cross-category correlations Z.pm <- SparseSpearmanCor2(s.prots, s.primary) Z.pl <- SparseSpearmanCor2(s.prots, s.lipid) Z.pa <- SparseSpearmanCor2(s.prots, s.amines) Z.ml <- SparseSpearmanCor2(s.primary, s.lipid) Z.ma <- SparseSpearmanCor2(s.primary, s.amines) Z.la <- SparseSpearmanCor2(s.lipid, s.amines) # Add row and column names to each dataframe dimnames(Z.pp) <- list(colnames(all_df[,cat.prots]), colnames(all_df[,cat.prots])) dimnames(Z.mm) <- list(colnames(all_df[,cat.primary]), colnames(all_df[,cat.primary])) dimnames(Z.ll) <- list(colnames(all_df[,cat.lipid]), colnames(all_df[,cat.lipid])) dimnames(Z.aa) <- list(colnames(all_df[,cat.amines]), colnames(all_df[,cat.amines]))  dimnames(Z.pm) <- list(colnames(all_df[,cat.prots]), colnames(all_df[,cat.primary])) dimnames(Z.pl) <- list(colnames(all_df[,cat.prots]), colnames(all_df[,cat.lipid])) dimnames(Z.pa) <- list(colnames(all_df[,cat.prots]), colnames(all_df[,cat.amines])) dimnames(Z.ml) <- list(colnames(all_df[,cat.primary]), colnames(all_df[,cat.lipid])) dimnames(Z.ma) <- list(colnames(all_df[,cat.primary]), colnames(all_df[,cat.amines])) dimnames(Z.la) <- list(colnames(all_df[,cat.lipid]), colnames(all_df[,cat.amines]))"},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"within-category-standarized-distributions","dir":"Articles","previous_headings":"WGCNA, Integration of multi-omics > Generate Correlations","what":"Within-category standarized distributions","title":"standardcor-WGCNA","text":"use standardcor functions estimate Beta parameters mean std dev omic.  blue line shows model distrubtion. can see fits background distribution. repeat process omics","code":"### Protein Protein Z.unique <- Z.pp[row(Z.pp) < col(Z.pp)] vw <- estimateShape(Z.pp) v.pp <- vw[1] w.pp <- vw[2] print(paste(\"Protein pairs: rho_ij ~ Beta(v =\",round(v.pp,3),\",w =\",round(w.pp,3),\")\")) #> [1] \"Protein pairs: rho_ij ~ Beta(v = 34.98 ,w = 34.002 )\"  fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,4),      main=\"Pairwise protein correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, v.pp, w.pp)/2, lwd=3, col=\"MediumBlue\") ### Metabolite-Metabolite Z.unique <- as.vector(Z.mm[row(Z.mm) < col(Z.mm)]) vw <- estimateShape(Z.mm) v.mm <- vw[1] w.mm <- vw[2] print(paste(\"Metabolite Pairs: rho_ij ~ Beta(v =\",round(v.mm,3),\",w =\",round(w.mm,3),\")\")) #> [1] \"Metabolite Pairs: rho_ij ~ Beta(v = 98.489 ,w = 98.706 )\"  fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise primary metabolite correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, v.mm, w.mm)/2, lwd=3, col=\"MediumBlue\") ### Lipid - Lipid Z.unique <- as.vector(Z.ll[row(Z.ll) < col(Z.ll)]) vw <- estimateShape(Z.ll) v.ll <- vw[1] w.ll <- vw[2] print(paste(\"Lipid Pairs: rho_ij ~ Beta(v =\",round(v.ll,3),\",w =\",round(w.ll,3),\")\")) #> [1] \"Lipid Pairs: rho_ij ~ Beta(v = 27.938 ,w = 25.131 )\"  fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise lipid correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, v.ll, w.ll)/2, lwd=3, col=\"MediumBlue\") ### Amine - Amine Z.unique <- as.vector(Z.aa[row(Z.aa) < col(Z.aa)]) vw <- estimateShape(Z.aa) v.aa <- vw[1] w.aa <- vw[2] print(paste(\"Amine Pairs: rho_ij ~ Beta(v =\",round(vw[1],3),\",w =\",round(vw[2],3),\")\")) #> [1] \"Amine Pairs: rho_ij ~ Beta(v = 112.787 ,w = 109.907 )\"  fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise amine correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, vw[1], vw[2])/2, lwd=3, col=\"MediumBlue\")"},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"cross-category","dir":"Articles","previous_headings":"","what":"Cross-category","title":"standardcor-WGCNA","text":"continue process cross-category omics combinations. consider.","code":"dim(Z.pm) #> [1] 283 149 Z.unique <- as.vector(Z.pm) # there are no self-comparisons, nor are there repeats due to symmetry vw <- estimateShape(Z.pm) v.pm <- vw[1] w.pm <- vw[2] print(paste(\"Protein-metabolite: rho_ij ~ Beta(v =\",round(vw[1],3),\",w =\",round(vw[2],3),\")\")) #> [1] \"Protein-metabolite: rho_ij ~ Beta(v = 174.852 ,w = 173.513 )\"  # The distribution of these cross-correlations is # markedly narrower than either of the contributing 'omics fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise Protein-metabolite correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, vw[1], vw[2])/2, lwd=3, col=\"MediumBlue\") # Modeling cross-category correlations: protein-lipid Z.unique <- as.vector(Z.pl) # there are no self-comparisons, nor are there repeats due to symmetry vw <- estimateShape(Z.pl) v.pl <- vw[1] w.pl <- vw[2] print(paste(\"Protein-metabolite: rho_ij ~ Beta(v =\",round(vw[1],3),\",w =\",round(vw[2],3),\")\")) #> [1] \"Protein-metabolite: rho_ij ~ Beta(v = 87.319 ,w = 89.012 )\"  # The distribution of these cross-correlations is # markedly narrower than either of the contributing 'omics fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise Protein-metabolite correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, vw[1], vw[2])/2, lwd=3, col=\"MediumBlue\") # Modeling cross-category correlations: protein-amine Z.unique <- as.vector(Z.pa) # there are no self-comparisons, nor are there repeats due to symmetry vw <- estimateShape(Z.pa) v.pa <- vw[1] w.pa <- vw[2] print(paste(\"Protein-metabolite: rho_ij ~ Beta(v =\",round(vw[1],3),\",w =\",round(vw[2],3),\")\")) #> [1] \"Protein-metabolite: rho_ij ~ Beta(v = 136.503 ,w = 136.271 )\"  # The distribution of these cross-correlations is # markedly narrower than either of the contributing 'omics fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise Protein-metabolite correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, vw[1], vw[2])/2, lwd=3, col=\"MediumBlue\") # Modeling cross-category correlations: primary-lipid Z.unique <- as.vector(Z.ml) # there are no self-comparisons, nor are there repeats due to symmetry vw <- estimateShape(Z.ml) v.ml <- vw[1] w.ml <- vw[2] print(paste(\"Protein-lipid: rho_ij ~ Beta(v =\",round(vw[1],3),\",w =\",round(vw[2],3),\")\")) #> [1] \"Protein-lipid: rho_ij ~ Beta(v = 184.501 ,w = 185.53 )\"  # The distribution of these cross-correlations is # markedly narrower than either of the contributing 'omics fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise Protein-lipid correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, vw[1], vw[2])/2, lwd=3, col=\"MediumBlue\") # Modeling cross-category correlations: primary-amine Z.unique <- as.vector(Z.ma) # there are no self-comparisons, nor are there repeats due to symmetry vw <- estimateShape(Z.ma) v.ma <- vw[1] w.ma <- vw[2] print(paste(\"Protein-amine: rho_ij ~ Beta(v =\",round(vw[1],3),\",w =\",round(vw[2],3),\")\")) #> [1] \"Protein-amine: rho_ij ~ Beta(v = 184.687 ,w = 184.458 )\"  # The distribution of these cross-correlations is # markedly narrower than either of the contributing 'omics fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise Protein-amine correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, vw[1], vw[2])/2, lwd=3, col=\"MediumBlue\") # Modeling cross-category correlations: lipid-amine Z.unique <- as.vector(Z.la) # there are no self-comparisons, nor are there repeats due to symmetry vw <- estimateShape(Z.la) v.la <- vw[1] w.la <- vw[2] print(paste(\"Lipid-amine: rho_ij ~ Beta(v =\",round(vw[1],3),\",w =\",round(vw[2],3),\")\")) #> [1] \"Lipid-amine: rho_ij ~ Beta(v = 125.539 ,w = 125.208 )\"  # The distribution of these cross-correlations is # markedly narrower than either of the contributing 'omics fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise lipid-amine correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, vw[1], vw[2])/2, lwd=3, col=\"MediumBlue\")"},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"merging-correlations-from-disparate-data-subsets","dir":"Articles","previous_headings":"Cross-category","what":"Merging correlations from disparate data subsets","title":"standardcor-WGCNA","text":"centering process similar , â€œquantile normalizationâ€. quantile normalization, value xix_i observed data cumulative probability pi=Pr{x<xi}p_i = \\Pr{\\{x < x_i\\} } among observed data {x}\\{ x \\} , accounting possibility ties finite dataset, average rank observed values equal xix_i divided total number observed values. value xix_i transformed value qiq_i cumulative probability pip_i according normalizing probability distribution DD (.e.Â Pr{d<qi|D}=pi\\Pr{ \\{ d < q_i | D \\} } = p_i). centering process, rather using rank xix_i observed data, use null model observed data; pip_i therefore p-value xix_i null model, transformed value qiq_i significance normalized probability distribution DD xix_i original null distribution. allows us merge datasets retaining significance original cohort. empirical distribution used null model two processes . , use beta distribution null model precisely appropriate null model correlations arbitrary independent vectors , reasonable assumptions, indistinguishable Beta distribution. believe appropriate null model correlations, suggest primary effect null model account effective number dimensions observed data, prior using 2D geometric model compute correlations observations.  now standardized correlations. mean variance distribution suggest model (shown blue) fits less well standardizing model (orange); consequence differences models fitted empirical distributions, indicates enrichment high correlations observed individual â€™omics distributions preserved. used quantile normalization, overabundance high correlations shifted lower correlation values, fitted blue model identical standardizing model.","code":"nu.std <- 34 # As wide as the widest compoonent, and centered at 0  Zc.pp <- centerBeta(Z.pp, v.pp, w.pp, nu.std) Zc.mm <- centerBeta(Z.mm, v.mm, w.mm, nu.std) Zc.ll <- centerBeta(Z.ll, v.ll, w.ll, nu.std) Zc.aa <- centerBeta(Z.aa, v.aa, w.aa, nu.std) Zc.pm <- centerBeta(Z.pm, v.pm, w.pm, nu.std) Zc.pl <- centerBeta(Z.pl, v.pl, w.pl, nu.std) Zc.pa <- centerBeta(Z.pa, v.pa, w.pa, nu.std) Zc.ml <- centerBeta(Z.ml, v.ml, w.ml, nu.std) Zc.ma <- centerBeta(Z.ma, v.ma, w.ma, nu.std) Zc.la <- centerBeta(Z.la, v.la, w.la, nu.std) # Combined, centered correlations  Zc <- matrix(0, nrow = length(all.analytes),              ncol = length(all.analytes)) rownames(Zc) <- all.analytes colnames(Zc) <- all.analytes ### Construct a final dataframe that contains all the correlation values. ### # Block-structured correlation matrix # Zc = [ PP     PM   PL  PA  | #      | PM^T   MM   ML   MA | #      | PL^T  MC^T  LL  LA  | #      | PA^T  MA^T  LA^T AA ] ### Zc[cat.prots, cat.prots] <- Zc.pp Zc[cat.primary,  cat.primary]  <- Zc.mm Zc[cat.lipid,  cat.lipid]  <- Zc.ll Zc[cat.amines,  cat.amines]  <- Zc.aa  Zc[cat.prots, cat.primary]  <- Zc.pm Zc[cat.primary, cat.prots]  <- t(Zc.pm)  Zc[cat.prots, cat.lipid]  <- Zc.pl Zc[cat.lipid, cat.prots]  <- t(Zc.pl)  Zc[cat.prots, cat.amines]  <- Zc.pa Zc[cat.amines, cat.prots]  <- t(Zc.pa)  Zc[cat.primary, cat.lipid]  <- Zc.ml Zc[cat.lipid, cat.primary]  <- t(Zc.ml)  Zc[cat.primary, cat.amines]  <- Zc.ma Zc[cat.amines, cat.primary]  <- t(Zc.ma)  Zc[cat.lipid, cat.amines]  <- Zc.la Zc[cat.amines, cat.lipid]  <- t(Zc.la)   print(str_c(\"nrow: \", nrow(Zc))) #> [1] \"nrow: 1465\" Zc[1:5,1:5] #>              1433Z        A1AG2        A1AT         A1BG        A2AP #> 1433Z  1.000000000  0.005707413  0.02901604 -0.006209182 -0.28999711 #> A1AG2  0.005707413  1.000000000  0.12406421  0.087104661 -0.17529426 #> A1AT   0.029016044  0.124064205  1.00000000  0.101222530 -0.29220877 #> A1BG  -0.006209182  0.087104661  0.10122253  1.000000000 -0.03940435 #> A2AP  -0.289997110 -0.175294264 -0.29220877 -0.039404352  1.00000000 Z.unique <- Zc[row(Zc) < col(Zc)] print(paste(\"Target: rho_ij ~ Beta(v =\",round(nu.std,3),\",w =\",round(nu.std,3),\")\")) #> [1] \"Target: rho_ij ~ Beta(v = 34 ,w = 34 )\"  x <- (1+Z.unique)/2 mZ <- mean(x) s2Z <- var(x) v.c <- mZ*(mZ*(1-mZ)/s2Z - 1) w.c <- (1-mZ)*(mZ*(1-mZ)/s2Z - 1) print(paste(\"Method of moments: rho_ij ~ Beta(v =\",round(v.c,3),\",w =\",round(w.c,3),\")\")) #> [1] \"Method of moments: rho_ij ~ Beta(v = 22.515 ,w = 21.686 )\"   fine <- 100 Zc.unique <- as.vector(Zc[row(Zc) < col(Zc)]) Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Zc.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,5),      main=\"All pairwise correlations, centered\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, nu.std, nu.std)/2, lwd=3, col=\"orangered\") lines(r, dbeta((1+r)/2, v.c, w.c)/2, lwd=3, col=\"MediumBlue\")"},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"wgcna","dir":"Articles","previous_headings":"","what":"WGCNA","title":"standardcor-WGCNA","text":"code follows standard WGCNA analysis. information can found : https://peterlangfelder.com/2018/11/25/wgcna-resources---web/       Now identified modules asssociated phenotype, can take analysis many directions. Please refer WGCNA documentation information.","code":"#Manually convert the pairwise correlation DF to the signed network DF Zc_signed <- 0.5 + 0.5 * Zc  print(str_c(\"nrow: \", nrow(Zc_signed))) #> [1] \"nrow: 1465\" #Choose a set of soft-thresholding powers powers <- c(c(1:10), seq(from=11, to=15, by=1)) cutoff <- 0.8  #Call the network topology analysis function sft <- pickSoftThreshold.fromSimilarity(Zc_signed, RsquaredCut=cutoff, powerVector=powers, blockSize=5000, verbose=5) #>  pickSoftThreshold: calculating connectivity for given powers... #>    ..working on genes 1 through 1465 of 1465 #> Warning: executing %dopar% sequentially: no parallel backend registered #>    Power SFT.R.sq  slope truncated.R.sq mean.k. median.k. max.k. #> 1      1 0.167000 12.400          0.944  746.00   745.000  806.0 #> 2      2 0.172000  5.280          0.950  388.00   385.000  459.0 #> 3      3 0.000524  0.143          0.772  206.00   202.000  276.0 #> 4      4 0.305000 -2.210          0.730  113.00   108.000  181.0 #> 5      5 0.705000 -2.770          0.816   63.10    58.700  126.0 #> 6      6 0.910000 -2.870          0.918   36.60    32.600   94.2 #> 7      7 0.957000 -2.590          0.948   22.00    18.300   74.3 #> 8      8 0.936000 -2.350          0.918   13.80    10.500   61.3 #> 9      9 0.963000 -2.060          0.952    9.15     6.140   52.5 #> 10    10 0.965000 -1.850          0.955    6.38     3.660   46.4 #> 11    11 0.960000 -1.710          0.948    4.70     2.250   41.9 #> 12    12 0.954000 -1.600          0.942    3.65     1.420   38.5 #> 13    13 0.963000 -1.500          0.955    2.97     0.895   35.9 #> 14    14 0.974000 -1.430          0.970    2.52     0.581   33.9 #> 15    15 0.963000 -1.390          0.957    2.21     0.380   32.4  #Plot the results options(repr.plot.width=9, repr.plot.height=5) par(mfrow=c(1,2)) cex1 <- 0.8 ##Scale-free topology fit index as a function of the soft-thresholding power plot(sft$fitIndices[,1], -sign(sft$fitIndices[,3])*sft$fitIndices[,2],      xlab=\"Soft Threshold (power)\", ylab=\"Scale Free Topology Model Fit, signed R^2\", type=\"n\",      main=paste(\"Scale independence\")) text(sft$fitIndices[,1], -sign(sft$fitIndices[,3])*sft$fitIndices[,2],      labels=powers, cex=cex1, col=\"black\") ##Line corresponds to using an R^2 cut-off of h abline(h=cutoff, col=\"red\") ##Mean connectivity as a function of the soft-thresholding power plot(sft$fitIndices[,1], sft$fitIndices[,5],      xlab=\"Soft Threshold (power)\", ylab=\"Mean Connectivity\", type=\"n\",      main=paste(\"Mean connectivity\")) text(sft$fitIndices[,1], sft$fitIndices[,5], labels=powers, cex=cex1, col=\"black\") print(str_c(\"Estimated soft-thresholding power: \", sft$powerEstimate)) #> [1] \"Estimated soft-thresholding power: 6\" #Choose the power that best approximates a scale free topology while still maintaining high level of connectivity in the network softPower <- sft$powerEstimate softPower #> [1] 6 #Generate the adjacency matrix using the chosen soft-thresholding power adjacency <- adjacency.fromSimilarity(Zc, power=softPower, type=\"signed\")  print(str_c(\"nrow: \", nrow(adjacency))) #> [1] \"nrow: 1465\" #head(adjacency)  #Turn adjacency into topological overlap ##You can input whatever matrix you want here! # Turn adjacency into topological overlap TOM = TOMsimilarity(adjacency,TOMType = \"signed\"); #> ..connectivity.. #> ..matrix multiplication (system BLAS).. #> ..normalization.. #> ..done. # Turn into distance matrix dissTOM = 1-TOM colnames(dissTOM) <- colnames(all_df) rownames(dissTOM) <- colnames(dissTOM) # Cluster the TOM distance matrix to find modules # Can call whatever clusting method you want here  # Call the hierarchical clustering function geneTree = hclust(as.dist(dissTOM), method = \"ward.D2\"); # Plot the resulting clustering tree (dendrogram) #sizeGrWindow(12,9) plot(geneTree, xlab=\"\", sub=\"\", main = \"Gene clustering on TOM-based dissimilarity\",     labels = FALSE, hang = 0.04); box() #Larger modules can be easier to interpret, so we set the minimum module size relatively high minModuleSize <- max(c(20, round(ncol(all_df)/200, digits=0))) print(str_c(\"minClusterSize = \", minModuleSize)) #> [1] \"minClusterSize = 20\"  #Module identification using dynamic tree cut dynamicMods <- cutreeDynamic(dendro=geneTree, distM=dissTOM,                              deepSplit=4, pamStage=TRUE, pamRespectsDendro=FALSE,                              minClusterSize=minModuleSize) #>  ..cutHeight not given, setting it to 4.94  ===>  99% of the (truncated) height range in dendro. #>  ..done. table(dynamicMods) #> dynamicMods #>   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  #> 499 104  94  78  72  63  60  54  48  47  43  39  32  30  29  28  26  26  25  25  #>  21  22  #>  22  21  #Convert numeric lables into colors dynamicColors <- labels2colors(dynamicMods) table(dynamicColors) #> dynamicColors #>        black         blue        brown         cyan    darkgreen      darkred  #>           60          104           94           30           21           22  #>        green  greenyellow       grey60    lightcyan   lightgreen  lightyellow  #>           72           43           26           28           26           25  #>      magenta midnightblue         pink       purple          red    royalblue  #>           48           29           54           47           63           25  #>       salmon          tan    turquoise       yellow  #>           32           39          499           78  #Plot the dendrogram and colors underneath options(repr.plot.width=12, repr.plot.height=6) plotDendroAndColors(geneTree, dynamicColors, \"Dynamic Tree Cut\",                     dendroLabels=FALSE, hang=0.03,                     addGuide=TRUE, guideHang=0.05,                     main=\"Gene dendrogram and module colors\") #Calculate eigengenes MEList <- moduleEigengenes(all_df, colors=dynamicColors, impute=TRUE, nPC=2) MEs <- MEList$eigengenes print(str_c(\"nrow: \", nrow(MEs))) #> [1] \"nrow: 500\" head(MEs) #>       MEblack       MEblue      MEbrown       MEcyan  MEdarkgreen     MEdarkred #> 1 -0.04250164 -0.006137058 -0.061119212 -0.017963073  0.083685677  2.766099e-02 #> 2  0.07743639  0.062143054 -0.024485473  0.030683603 -0.021758338 -4.796828e-05 #> 3  0.03539161  0.026002054 -0.073829551  0.011904043  0.006682852  2.677434e-02 #> 4 -0.01816194 -0.021918239  0.006197687 -0.005365383 -0.030132389  1.233884e-02 #> 5  0.01788601  0.014131597 -0.090108419  0.041251157  0.012587136 -6.189113e-02 #> 6  0.02603770  0.001758729 -0.043114754  0.024930105  0.001555772 -1.455030e-02 #>       MEgreen MEgreenyellow     MEgrey60 MElightcyan  MElightgreen #> 1  0.09667654    0.04224804 -0.011219413  0.02270412 -0.0009493078 #> 2  0.01144213    0.06930301  0.011247960  0.03394327  0.0926655585 #> 3  0.03526924    0.04939254  0.005733386  0.02926117  0.0418565133 #> 4 -0.02069531    0.02095189  0.032070655 -0.00474506 -0.0296049856 #> 5  0.06652515   -0.01577464 -0.105567516  0.02761338  0.0080213115 #> 6  0.06117528    0.01558672 -0.039090274  0.02622454 -0.0552738630 #>   MElightyellow    MEmagenta MEmidnightblue       MEpink     MEpurple #> 1   0.054813453  0.017014383   -0.060639183 -0.041704707  0.015084532 #> 2   0.006505377  0.021206367    0.041181322  0.016637743  0.070252083 #> 3   0.028960827  0.004705876   -0.013260868 -0.049713898  0.067356769 #> 4   0.010579150  0.037737960   -0.009881932  0.001515955 -0.039509676 #> 5   0.012931648 -0.018482246    0.068292959 -0.094043994 -0.038940804 #> 6  -0.021155760  0.039405631    0.053630301 -0.039897490  0.002201621 #>          MEred  MEroyalblue   MEsalmon         MEtan MEturquoise     MEyellow #> 1  0.004533408  0.056728907 0.01880385  0.0104194410 -0.03167811 -0.007773314 #> 2  0.022852213 -0.027008752 0.04014007 -0.0451096975  0.01457797 -0.013980877 #> 3  0.065539735  0.034050318 0.03833421 -0.0105466068 -0.05322893 -0.037205440 #> 4  0.005094828  0.003775484 0.01594035 -0.0039805473  0.01949922 -0.013305095 #> 5 -0.056656398 -0.091232433 0.01890705 -0.0649936854 -0.02318406 -0.118914485 #> 6  0.055026948 -0.008089793 0.04426588 -0.0002465442 -0.06392858 -0.075245659  #Calculate dissimilarity of module eigengenes MEDiss <- 1 - cor(MEs, use=\"pairwise.complete.obs\")  #Cluster module eigengenes METree <- hclust(as.dist(MEDiss), method=\"ward.D2\")  #Plot the result options(repr.plot.width=10, repr.plot.height=5) plot(METree, main=\"Clustering of module eigengenes\",      xlab=\"\", sub=\"\") MEDissThres <- 0.3 abline(h=MEDissThres, col=\"red\") #Call an automatic merging function merge <- mergeCloseModules(all_df, dynamicColors, cutHeight=MEDissThres, verbose=0)  #Eigengenes of the new merged modules mergedMEs <- merge$newMEs  #The merged module colors mergedColors <- merge$colors table(mergedColors) #> mergedColors #>        blue       brown   darkgreen     darkred       green greenyellow  #>         223          94          46          22          72          43  #>      grey60   lightcyan  lightgreen     magenta        pink      purple  #>          26          60          26          48          54          47  #>         red   royalblue         tan   turquoise      yellow  #>          63          25          39         499          78  #Plot the dendrogram and module colors options(repr.plot.width=12, repr.plot.height=6) plotDendroAndColors(geneTree, cbind(dynamicColors, mergedColors),                     c(\"Dynamic Tree Cut\", \"Merged dynamic\"),                     dendroLabels=FALSE, hang=0.03,                     addGuide=TRUE, guideHang=0.05,                     main=\"Gene dendrogram and module colors\") #Rename moduleColors <- mergedColors MEs <- mergedMEs #Rename moduleColors <- mergedColors MEs <- mergedMEs  #Clean the module eigengene table eigengene_df <- MEs %>%     rownames_to_column(var=\"public_client_id\") names(eigengene_df)[2:ncol(eigengene_df)] <- names(eigengene_df)[2:ncol(eigengene_df)] %>%     str_replace(., \"^ME\", \"\") %>%     str_to_title(.) print(\"Module eigengene table\") #> [1] \"Module eigengene table\" print(str_c(\"- nrow: \", nrow(eigengene_df))) #> [1] \"- nrow: 500\" head(eigengene_df) #>   public_client_id       Green   Turquoise Greenyellow   Lightcyan        Blue #> 1                1  0.09667654 -0.03167811  0.04224804 0.021016558 -0.03960362 #> 2                2  0.01144213  0.01457797  0.06930301 0.038385247  0.05453270 #> 3                3  0.03526924 -0.05322893  0.04939254 0.035184316  0.01207804 #> 4                4 -0.02069531  0.01949922  0.02095189 0.005809384 -0.01353862 #> 5                5  0.06652515 -0.02318406 -0.01577464 0.024391282  0.04479661 #> 6                6  0.06117528 -0.06392858  0.01558672 0.038461457  0.03384631 #>      Darkgreen       Darkred    Royalblue           Tan          Red #> 1  0.072203330  2.766099e-02  0.056728907  0.0104194410  0.004533408 #> 2 -0.007447819 -4.796828e-05 -0.027008752 -0.0451096975  0.022852213 #> 3  0.018880591  2.677434e-02  0.034050318 -0.0105466068  0.065539735 #> 4 -0.009537316  1.233884e-02  0.003775484 -0.0039805473  0.005094828 #> 5  0.013223197 -6.189113e-02 -0.091232433 -0.0649936854 -0.056656398 #> 6 -0.010730048 -1.455030e-02 -0.008089793 -0.0002465442  0.055026948 #>        Magenta    Lightgreen       Purple        Brown         Pink #> 1  0.017014383 -0.0009493078  0.015084532 -0.061119212 -0.041704707 #> 2  0.021206367  0.0926655585  0.070252083 -0.024485473  0.016637743 #> 3  0.004705876  0.0418565133  0.067356769 -0.073829551 -0.049713898 #> 4  0.037737960 -0.0296049856 -0.039509676  0.006197687  0.001515955 #> 5 -0.018482246  0.0080213115 -0.038940804 -0.090108419 -0.094043994 #> 6  0.039405631 -0.0552738630  0.002201621 -0.043114754 -0.039897490 #>         Grey60       Yellow #> 1 -0.011219413 -0.007773314 #> 2  0.011247960 -0.013980877 #> 3  0.005733386 -0.037205440 #> 4  0.032070655 -0.013305095 #> 5 -0.105567516 -0.118914485 #> 6 -0.039090274 -0.075245659 ##Sample metadata sample_tbl <- pheno[pheno$subjectID %in% rownames(MEs),] print(\"Sample metadata after the filter\") #> [1] \"Sample metadata after the filter\" print(str_c(\"- nrow: \", nrow(sample_tbl))) #> [1] \"- nrow: 500\"  #Code sex and race phenotype_tbl <- sample_tbl   phenotype_tbl <- phenotype_tbl[match(rownames(MEs), rownames(phenotype_tbl)),] #Calculate the numbers of modules and samples #nModules <- ncol(MEs) nSamples <- nrow(phenotype_tbl)  #Names (colors) of the modules modNames = substring(names(MEs), 3)  ##Check ID order before the cor() function print(str_c(\"Matched IDs?: \", all(rownames(MEs)==rownames(phenotype_tbl)))) #> [1] \"Matched IDs?: TRUE\"  #Calculate moduleâ€“trait relationship moduleTraitCor <- as.data.frame(cor(MEs, phenotype_tbl, use=\"p\")) rownames(moduleTraitCor) <- str_to_title(modNames) print(\"Moduleâ€“trait relationship table\") #> [1] \"Moduleâ€“trait relationship table\" print(str_c(\"nrow: \", nrow(moduleTraitCor))) #> [1] \"nrow: 17\"  #Calculate statisitcal significance of moduleâ€“trait relationship MTRpval <- as.data.frame(corPvalueStudent(as.matrix(moduleTraitCor), nSamples)) rownames(MTRpval) <- str_to_title(modNames) print(\"Moduleâ€“trait relationship p-value table\") #> [1] \"Moduleâ€“trait relationship p-value table\" print(str_c(\"- nrow: \", nrow(MTRpval))) #> [1] \"- nrow: 17\"  #Eliminate the dummy module (Grey) moduleTraitCor <- moduleTraitCor[rownames(moduleTraitCor)!=\"Grey\",] MTRpval <- MTRpval[rownames(MTRpval)!=\"Grey\",]  #P-value adjustment across modules (per trait) using Benjaminiâ€“Hochberg method MTRpval_adj <- as.data.frame(apply(MTRpval, 2, function(x){p.adjust(x, length(x), method=\"BH\")})) print(\"Moduleâ€“trait relationship adjusted p-value table\") #> [1] \"Moduleâ€“trait relationship adjusted p-value table\" print(str_c(\"- nrow: \", nrow(MTRpval_adj))) #> [1] \"- nrow: 17\"  #Prepare text labels as matrix textMatrix <- paste(\"r = \",signif(as.matrix(moduleTraitCor), 3),\"\\n(P = \",                     signif(as.matrix(MTRpval_adj), 2),\")\", sep=\"\") dim(textMatrix) <- dim(moduleTraitCor) #Revert module names back to apply color conversion temp_c <- rownames(moduleTraitCor) %>%     str_to_lower(.) %>%     str_c(\"ME\",.) #Visualize options(repr.plot.width=10, repr.plot.height=10) par(mar=c(5, 5, 3, 2)) labeledHeatmap(Matrix=moduleTraitCor,                xLabels=colnames(moduleTraitCor),                yLabels=temp_c,                #ySymbols=rownames(moduleTraitCor),                colorLabels=FALSE,                colors=blueWhiteRed(50),                textMatrix=textMatrix,                setStdMargins=FALSE,                cex.text=1,                zlim=c(-1,1),                main=paste(\"Moduleâ€“trait relationships\"))"},{"path":"https://longevity-consortium.github.io/standardcor/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Max Robinson. Maintainer.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Robinson M, Pflieger L (2024). standardcor: Empirical Null Models Pairwise Correlationsâ€“Estimation Standardization. R package version 0.1.0, https://longevity-consortium.github.io/standardcor/.","code":"@Manual{,   title = {standardcor: Empirical Null Models of Pairwise Correlations--Estimation and Standardization},   author = {Max Robinson and Lance Pflieger},   year = {2024},   note = {R package version 0.1.0},   url = {https://longevity-consortium.github.io/standardcor/}, }"},{"path":"https://longevity-consortium.github.io/standardcor/index.html","id":"standardcor","dir":"","previous_headings":"","what":"Empirical Null Models of Pairwise Correlations--Estimation and Standardization","title":"Empirical Null Models of Pairwise Correlations--Estimation and Standardization","text":"package provides utilities interpreting correlation coefficients distances adjacency weights clustering. Pairwise correlations geometric interpretation cosine angle two vectors length N, important note regardless large N , angle inherently 2-dimensional concept. standard approaches convert pairwise correlations distances retain 2-dimensional behavior. pairs set vectors considered, general vectors similar planes different orientations, distribution across pairs reflect structure N-dimensional space. package therefore uses Beta distribution null model bulk, spurious correlations extend standard approaches, producing N-dimensional distances. null model also provides alternative, statistical approach defining adjacency weights first step constructing network model correlations Weighted Correlation Network Analysis. important contribution package, though, co-clustering datasets collected different high-throughput (â€™omics) technologies, distribution spurious correlations general different statistical characteristics. fitting distribution separately, package provides means standardize correlation values across different technologies, well cross-correlations data measured different technologies. standardization addresses differences variance spurious correlations (noise levels) across â€™omics platforms, reducing observed tendency correlations noisier platforms drive clustering correlations less noisy platforms, correlations across platforms. package therefore supports correlation-based clustering providing conceptual framework, methods fitting distribution spurious correlations, methods standardizing correlation values across â€™omics platforms, new methods converting correlations distances network adjacencies.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/index.html","id":"installation-in-r","dir":"","previous_headings":"","what":"Installation in R:","title":"Empirical Null Models of Pairwise Correlations--Estimation and Standardization","text":"","code":"if (!require(\"remotes\", quietly = TRUE))    install.packages(\"remotes\")  remotes::install_github(\"longevity-consortium/standardcor\")"},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparseSpearmanCor2.html","id":null,"dir":"Reference","previous_headings":"","what":"Sparse implementation of Spearman Correlation â€” SparseSpearmanCor2","title":"Sparse implementation of Spearman Correlation â€” SparseSpearmanCor2","text":"Computes Spearman correlations either among columns one sparse matrix (X) columns two sparse matrices (X, Y). implementation specific Spearman correlation highly efficient; efficiency results avoiding unnecessary computations involving ranks zero entries, well equivalence rank correlations ranks shifted constant. Copied without code changes, textual change comments, public blog repository     https://github.com/saketkc/blog/blob/main/2022-03-10/SparseSpearmanCorrelation2.ipynb owned Saket Choudhary Mumbai, India 23 Mar 2024","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparseSpearmanCor2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sparse implementation of Spearman Correlation â€” SparseSpearmanCor2","text":"","code":"SparseSpearmanCor2(X, Y = NULL, cov = FALSE)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparseSpearmanCor2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sparse implementation of Spearman Correlation â€” SparseSpearmanCor2","text":"X sparse data matrix Y second sparse data matrix. X provided, columns X correlated ; Y also specified, columns X cross-correlated columns Y. cov optional covariance matrix use computing correlations. Passed qlcMatrix::corSparse().","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparseSpearmanCor2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sparse implementation of Spearman Correlation â€” SparseSpearmanCor2","text":"matrix M Spearman correlations. .null(Y), M[,j] = cor(X[,],X[,j],method='s'), otherwise M[,j] = cor(X[,],Y[j],method='s'). implementation considerably faster others X Y sparse.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparsifiedRanks2.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace non-zero entries in a sparse matrix with non-zero ranks â€” SparsifiedRanks2","title":"Replace non-zero entries in a sparse matrix with non-zero ranks â€” SparsifiedRanks2","text":"Creates rank matrix sparse matrix X using following approach: 1. Use non-zero enries column calculate ranks 2. Add (z-1)/2 ranks (non-zero entries changed). z number zeros column Since entries shifted constant (zeros already shifted), covariance matrix shifted matrix rank matrix entire matrix (zeros also rank = (z+1)/2) z number zeros rank matrix can used calculate Spearman's correlation coefficient (via pearson correlation) Copied without code changes minor changes comments, public blog repository     https://github.com/saketkc/blog/blob/main/2022-03-10/SparseSpearmanCorrelation2.ipynb owned Saket Choudhary Mumbai, India 23 Mar 2024","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparsifiedRanks2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace non-zero entries in a sparse matrix with non-zero ranks â€” SparsifiedRanks2","text":"","code":"SparsifiedRanks2(X)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparsifiedRanks2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace non-zero entries in a sparse matrix with non-zero ranks â€” SparsifiedRanks2","text":"X sparse data matrix Y second sparse data matrix. X provided, columns X correlated ; Y also specified, columns X cross-correlated columns Y.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparsifiedRanks2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Replace non-zero entries in a sparse matrix with non-zero ranks â€” SparsifiedRanks2","text":"sparse matrix ranks non-zero entries","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/betaDistance.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Dimensionally-adjusted correlation distances â€” betaDistance","title":"Calculate Dimensionally-adjusted correlation distances â€” betaDistance","text":"standard (Euclidean) transformation correlation coefficients distances based 2-dimensional geometry plane formed correlated vectors origin. However, simulations random vectors demonstrate probability correlations random vectors reaching given correlation coefficient depends number dimensions spanned random vectors. function allows transforming correlation coefficients distances reflect null distribution, corresponds Beta distribution can estimated bulk correlations assumption majority correlations null model.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/betaDistance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Dimensionally-adjusted correlation distances â€” betaDistance","text":"","code":"betaDistance(r, v = 1, w = 1, mix = 1, unsigned = TRUE)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/betaDistance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Dimensionally-adjusted correlation distances â€” betaDistance","text":"r numeric object containing correlation coefficients v first shape parameter 2-parameter Beta distribution w second shape parameter 2-parameter Beta distribution mix relative weight squared Euclidean distance Beta distance penalty. mix = 0, Beta distance used correlation significantly beyond null distribution essentially zero distance. mix = 1, correlation corresponding zero distance +/-1, Beta distance acts penalty top squared Euclidean distance. intermediate values allowed, recommended. unsigned TRUE (default), computes distance association (|r|) instead correlation (r).","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/betaDistance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Dimensionally-adjusted correlation distances â€” betaDistance","text":"numeric object containing dimensionally-adjusted distances","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/centerBeta.html","id":null,"dir":"Reference","previous_headings":"","what":"Center correlations â€” centerBeta","title":"Center correlations â€” centerBeta","text":"null distribution model r ~ 2 Beta(v,w) - 1 centering (target) distribution r_centered ~ 2 Beta(nu, nu) - 1","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/centerBeta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Center correlations â€” centerBeta","text":"","code":"centerBeta(r, v, w, nu)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/centerBeta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Center correlations â€” centerBeta","text":"r correlation center v first parameter Beta distribution w second parameter Beta distribution nu parameter target Beta distribution","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/centerBeta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Center correlations â€” centerBeta","text":"centered correlation","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/estimateShape.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate parameters of a Beta distribution null model â€” estimateShape","title":"Estimate parameters of a Beta distribution null model â€” estimateShape","text":"function estimates shape parameters v w Beta(v,w) distribution fit bulk (background) distribution given set correlations. objective estimating process match density distribution mode breadth distribution around mode, goal estimating model bulk (null) distribution assumption correlation coefficents provided corSet contains modest number values drawn null distribution. Note possible provide infallible definition desired null model; method therefore necessarily heuristic provide satisfactory result cases. occurs, user encouraged use \"left\" \"right\" parameters needed provide satisfactory null model.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/estimateShape.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate parameters of a Beta distribution null model â€” estimateShape","text":"","code":"estimateShape(   corSet,   left = 0,   right = 0,   plot = FALSE,   fine = NULL,   trim = 0.01,   ... )"},{"path":"https://longevity-consortium.github.io/standardcor/reference/estimateShape.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate parameters of a Beta distribution null model â€” estimateShape","text":"corSet vector non-unique, non-self correlation coefficients. left adjustment estimated value w. parameter provided effect changing estimated v parameter can assessed plot. Note mean Beta(v,w) v/(v+w), adding w moves mean left. right adjustment estimated value v. Adding value v moves mean Beta distribution right; Adding v w decreases variance. plot TRUE, plot distribution shown, along fitted distribution Beta(v,w). ev ew estimated parameters, v = ev + right, w = ew + left. fine Half number bins histogram. fine = NULL (default), appropriate value 5 100 used. trim mean correlations estimated robustly making initial Beta model, trimming values quantile trim/2 quantile 1-trim/2 initial model (0 <= trim < 1). trimming reduces number correlations remain less five, trimming disabled full set correlations used.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/estimateShape.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate parameters of a Beta distribution null model â€” estimateShape","text":"vector c(v, w) containing two Beta parameters (see plot=TRUE ).","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/euclidDistance.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Euclidean distance â€” euclidDistance","title":"Calculate Euclidean distance â€” euclidDistance","text":"Calculates distance network correlation coefficient matrix using Euclidean function.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/euclidDistance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Euclidean distance â€” euclidDistance","text":"","code":"euclidDistance(mat, v = 1, w = 1, type = \"unsigned\")"},{"path":"https://longevity-consortium.github.io/standardcor/reference/euclidDistance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Euclidean distance â€” euclidDistance","text":"mat matrix correlation coefficients v first parameter beta distribution w second parameter beta distribution type type distance calculate","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/euclidDistance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Euclidean distance â€” euclidDistance","text":"matrix distances","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/interpolatedAdjacency.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a (standardized) correlation value r to an adjacency â€” interpolatedAdjacency","title":"Convert a (standardized) correlation value r to an adjacency â€” interpolatedAdjacency","text":"Interpolates adjacency value 0..1 correlation value -1 <= r <= 1 tabulated soft thresholding function.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/interpolatedAdjacency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a (standardized) correlation value r to an adjacency â€” interpolatedAdjacency","text":"","code":"interpolatedAdjacency(r, adjTable)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/interpolatedAdjacency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a (standardized) correlation value r to an adjacency â€” interpolatedAdjacency","text":"r numeric object containing (standardized) correlation coefficients adjTable tabulated soft-threshold adjacency function, produced nullModelAdjacencyTable()","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/interpolatedAdjacency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a (standardized) correlation value r to an adjacency â€” interpolatedAdjacency","text":"numeric object containing adjacencies correlation network model","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/multiOmicModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Spearman correlations and normalization distributions within and between multiple datasets â€” multiOmicModel","title":"Compute Spearman correlations and normalization distributions within and between multiple datasets â€” multiOmicModel","text":"Manages computation Spearman correlations within multiple, potentially large ('Omics) datasets, returning matrix correlations pairs analytes. listed dataset expected numeric matrix containing numeric data, however input list can contain either matrix (referred internal datasets) large datasets, name RDS file containing dataset (external datasets). approach handle unlimited amount data; least, internal datasets every pair external datasets must fit memory simultaneously. function computes null models, separated block-wise construction standardized correlation matrix analytes provide opportunity null model parameters computed adjusted needed prior use standardization.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/multiOmicModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Spearman correlations and normalization distributions within and between multiple datasets â€” multiOmicModel","text":"","code":"multiOmicModel(OmicsL, common = TRUE, min.samples = 5, annotate = FALSE, ...)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/multiOmicModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Spearman correlations and normalization distributions within and between multiple datasets â€” multiOmicModel","text":"OmicsL named list datasets. dataset may matrix (internal dataset) name RDS file containing matrix (external dataset). matrices interpreted rows representing samples columns representing analytes; analyte names must unique across datasets. common TRUE (default), samples common datasets used compute correlations. Otherwise, correlations computed across samples shared individual pair datasets. requiring correlations depend common set samples, setting common FALSE permits available data contribute cross-correlation; user decide appropriate. min.samples lower limit number samples analyte-analyte correlations may computed. parameter used ensure datasets overlap sufficiently samples provide least minimal ability compute correlations analytes. common set samples required across datasets, parameter minimum length common set samples; otherwise min.samples minimum number samples shared pair 'Omics datasets independently.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/multiOmicModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Spearman correlations and normalization distributions within and between multiple datasets â€” multiOmicModel","text":"list two parts: 'modelL', list Beta distributon null models raw correlation values every pair input ('Omics) datasets, 'analyteL', list analytes contributed 'Omics dataset. standardized correlation matrix can computed two lists using standardizeFromModel(), either directly adjustment individual null model parameters needed. rows columns final standardized correlation matrix labeled full set analytes listed analyteL, must unique; effort made render analyte names unique either functions.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/nullModelAdjacencyTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute soft threshold adjacency table for unsigned correlations from a symmetric Beta distribution null model â€” nullModelAdjacencyTable","title":"Compute soft threshold adjacency table for unsigned correlations from a symmetric Beta distribution null model â€” nullModelAdjacencyTable","text":"Tabulates soft thresholding function converting correlations directly adjacencies suitable network model set correlations among set analytes, example table transcriptomic, proteomic, metabolomic, 'Omics data. tabulated function uses background model random correlations expressed parameter nu: Prr | null model = Prx <= Beta(nu, nu) | x = (1+r)/2, estimate number correlations background model, estimate value r probability , given value misclassified, classified false negative. estimated number false negatives (FN) false positives (FP), probability FN / (FN + FP). serves appropriate soft-threshold function, probability 1/2 estimate number false negatives equals number false positives.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/nullModelAdjacencyTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute soft threshold adjacency table for unsigned correlations from a symmetric Beta distribution null model â€” nullModelAdjacencyTable","text":"","code":"nullModelAdjacencyTable(uniqueCor, v, scale = 2, bins = 100)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/nullModelAdjacencyTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute soft threshold adjacency table for unsigned correlations from a symmetric Beta distribution null model â€” nullModelAdjacencyTable","text":"uniqueCor numeric object containing full set non-self, unique correlation values consider. analyte-analyte correlation matrix Z, non-self, unique correlations can found using Z[row(Z) < col(Z)]. v parameter null model random correlations, Prr | null model = Prx <= Beta(v, v) | x = (1+r)/2. scale Center soft threshold relative estimated number correlations beyond fit null model. Defaults 2; higher values increase connectivity network model, lower values decrease .","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/nullModelAdjacencyTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute soft threshold adjacency table for unsigned correlations from a symmetric Beta distribution null model â€” nullModelAdjacencyTable","text":"data frame 2 columns, x y, tabulating y function x -1 <= x <= 1 0 <= y <= 1.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/powerDistance.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate power distance â€” powerDistance","title":"Calculate power distance â€” powerDistance","text":"Calculates distances WCNA correlation coefficient(s) using power function.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/powerDistance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate power distance â€” powerDistance","text":"","code":"powerDistance(r, k, unsigned = TRUE)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/powerDistance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate power distance â€” powerDistance","text":"r Correlation coefficient(s), -1 <= r <= 1. k power parameter. unsigned TRUE, function computes distances based unsigned associations (|r|), otherwise computes distances based signed correlations (r).","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/powerDistance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate power distance â€” powerDistance","text":"matrix distances","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/powerDistance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate power distance â€” powerDistance","text":"Correlation distance transformed follows: type = \"unsigned\"= 1 - abs(r)^k; type = \"signed\" = ((1 - r)/2)^k","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/sigmoidDistance.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate sigmoid distance â€” sigmoidDistance","title":"Calculate sigmoid distance â€” sigmoidDistance","text":"Calculates distances correlation coefficients using sigmoid function. WCNA converts correlations distances distances adjacencies construct adjacency matrix network model analyte relationships pairwise correlations; function used first step. distances can thought providing soft thresholding correlations. sigmoid function provides threshold parameter (tau0), specifies correlation value corresponding adjacency = 0.5; rate parameter (alpha > 0), controls \"hardness\" threshold.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/sigmoidDistance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate sigmoid distance â€” sigmoidDistance","text":"","code":"sigmoidDistance(r, alpha, tau0, unsigned = TRUE, stretch = FALSE)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/sigmoidDistance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate sigmoid distance â€” sigmoidDistance","text":"r Correlation coefficient(s) alpha alpha parameter sigmoid function (log slope tau0) tau0 tau0 parameter sigmoid function (correlation value corresponding distance = 0.5) stretch TRUE, distances rescaled range 0..1. (default: stretch=FALSE) type interpretation correlation coefficient signed (correlation) unsigned (association, default). value \"unsigned\" interpreted signed.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/sigmoidDistance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate sigmoid distance â€” sigmoidDistance","text":"distance(s) corresponding correlation coefficient(s)","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/standardcor-package.html","id":null,"dir":"Reference","previous_headings":"","what":"standardcor: Empirical Null Models of Pairwise Correlationsâ€“Estimation and Standardization â€” standardcor-package","title":"standardcor: Empirical Null Models of Pairwise Correlationsâ€“Estimation and Standardization â€” standardcor-package","text":"package addresses issues arising using correlation coefficients determine adjacencies network representations systems interacting variables, measured quantities gene transcripts, proteins, metabolites, microRNAs, lipids, related measurements collected high-throughput technologies employed molecular biological research. methods specific application, statistical intended practical value regardless field application; however, package motivated issues arising stated application.","code":""},{"path":[]},{"path":"https://longevity-consortium.github.io/standardcor/reference/standardizeFromModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Build a standardized correlation matrix using null models for each pair of component datasets â€” standardizeFromModel","title":"Build a standardized correlation matrix using null models for each pair of component datasets â€” standardizeFromModel","text":"Uses provided list null models (modelL) standardize type correlations target common null model specified v.std parameter, (r + 1)/2 ~ Beta(v.std, v.std), r standardized Spearman correlation coefficient two analytes modeled list 'Omics datasets. analtyes listed dataset components list parameter analtyeL, must unique contain analytes provided raw correlation matrix modelL. resulting standardized correlation matrix built blockwise full symmetric.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/standardizeFromModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build a standardized correlation matrix using null models for each pair of component datasets â€” standardizeFromModel","text":"","code":"standardizeFromModel(modelL, analyteL, v.std = 32)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/standardizeFromModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build a standardized correlation matrix using null models for each pair of component datasets â€” standardizeFromModel","text":"modelL list--lists structure providing two inputs pair 'Omics datasets B: matrix modelL[[]][[B]][['cor']] raw Spearman correlation values, shape parameters modelL[[]][[B]][['shape']] == c(v,w) specifying null model (1+r_raw)/2 ~ Beta(v,w) raw correlations. v.std  analtyeL 'Omics dataset , analyteL[[]] lists analytes provided dataset . analyte identifiers required unique across datasets.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/standardizeFromModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build a standardized correlation matrix using null models for each pair of component datasets â€” standardizeFromModel","text":"symmetric matrix containing standardized Spearman correlation coefficients every pair analytes across datasets.","code":""}]
