[{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"wgcna-integration-of-multi-omics","dir":"Articles","previous_headings":"","what":"WGCNA, Integration of multi-omics","title":"standardcor-WGCNA","text":"notebook covers correlation standardization technique (information : https://github.com/PriceLab/standardcor) combine omics downstream WGCNA analysis. combine omics, start correlations separately omics. omics tends different distribution correlations; example, randomly selected analytes proteomics assay tends positively correlated (also true transcriptomics), randomly selected metabolites typically closer uncorrelated. issue computing adjacencies single â€™omics, transformation correlations adjacencies (WGCNA) fit separate distributions separately. combine correlations across different â€™omics, analytes measured different â€™omics experiments, WGCNA attempt find one power ð‘˜ fit , result different distributions adjacency analyte pairs different source experiments. One particular source differences distribution sample size. compute correlations metabolites 1,000 individuals, expect much accurate correlation values compute 10; sampling variation alone result sqrt(1000/10) = 10-fold difference variance! therefore make smooth model distribution correlations type, transform correlation values single, shared smooth model. resulting values longer interpretable correlations, standardize significance correlations different sources onto single, shared significance scale. WGCNA fits values scale, applying significance standards correlations, regardless original source.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"data-and-preprocessing","dir":"Articles","previous_headings":"WGCNA, Integration of multi-omics","what":"Data and preprocessing","title":"standardcor-WGCNA","text":"required input files : Phenotype Table - containing outcome inPrimary Merest MetabolitPrimary e Table - metabolite abunda Lipid Table - Lipid abundance values Biogenic Amine Table - Biogenic Amine Tablence values Protein Table - protein abundance values Data analysis synthesized Longevity Consortium generated proteomic metabolomic data. used Gaussian Mixture Model (GMM) create 1000 synthetic participants omic type, 500 cases 500 controls, followed addition synthetic case/control signal. signal generated adding 0.1 set proteins metabolites cases. original Longevity Consortium datasets filtered high missingness (>20%), imputed using random-forest imputation log normalized prior running GMM.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"load-data","dir":"Articles","previous_headings":"WGCNA, Integration of multi-omics","what":"Load data","title":"standardcor-WGCNA","text":"","code":"#Phenotypes pheno <- read_delim(\"../data/WGCNA/case_synth.tsv\", show_col_types = FALSE)  ### Load proteins prots <- read_delim(\"../data/WGCNA/proteins_synth.tsv\", show_col_types = FALSE)  ### Metabolites lipids_df <- read_delim(\"../data/WGCNA/Lipids_synth.tsv\", show_col_types = FALSE) amines_df  <- read_delim(\"../data/WGCNA/BA_synth.tsv\", show_col_types = FALSE) primary_df  <- read_delim(\"../data/WGCNA/Primary_synth.tsv\", show_col_types = FALSE)   ### Metabolite features features <- read_delim(\"../data/WGCNA/Met_Features.tsv\", show_col_types = FALSE) ## Merge together in_df <- merge(primary_df, lipids_df, by=\"subjectID\") in_df <- merge(in_df, amines_df, by=\"subjectID\") in_df <- merge(in_df, prots, by=\"subjectID\")  # Check dimensions of each dataframe dim(primary_df) #> [1] 500 150 dim(lipids_df) #> [1] 500 713 dim(amines_df) #> [1] 500 322 dim(prots) #> [1] 500 284 dim(in_df) #> [1]  500 1466 dim(pheno) #> [1] 500   2 # Drop id column and get features num.analytes <- setdiff(unique(c(colnames(primary_df),colnames(lipids_df),colnames(amines_df),colnames(prots))),'subjectID') num_df <- in_df[,colnames(in_df) %in% num.analytes] num_df <- as.matrix(num_df) rownames(num_df) <- in_df$subjectID ## Filter samples and features based on WGCNA NA criteria (50%) gsg = goodSamplesGenes(num_df, verbose = 5); #>  Flagging genes and samples with too many missing values... #>   ..step 1 gsg$allOK #> [1] TRUE if (!gsg$allOK) {   # Optionally, print the gene and sample names that were removed:   if (sum(!gsg$goodGenes)>0)      printFlush(paste(\"Removing genes:\", paste(names(num_df)[!gsg$goodGenes], collapse = \", \")));   if (sum(!gsg$goodSamples)>0)      printFlush(paste(\"Removing samples:\", paste(rownames(num_df)[!gsg$goodSamples], collapse = \", \")));   # Remove the offending genes and samples from the data:   num_df = num_df[gsg$goodSamples, gsg$goodGenes] }  dim(num_df) #> [1]  500 1465 # Get the names of remaining analytes overall (all.analytes) by category cat.prots <- intersect(colnames(prots),colnames(num_df)) cat.primary  <- intersect(colnames(primary_df),colnames(num_df)) cat.lipid <- intersect(colnames(lipids_df),colnames(num_df)) cat.amines <- intersect(colnames(amines_df),colnames(num_df))  all.analytes <- c(cat.prots,cat.primary,cat.lipid,cat.amines) print(paste(length(cat.prots),length(cat.lipid),length(cat.primary),length(cat.amines),length(all.analytes))) #> [1] \"283 712 149 321 1465\"  # We will construct a correlation matrix Z in parts corresponding to each category of analyte pairs. n.analytes <- length(all.analytes) # To compute correlations, features must be numeric. We will use Spearman all_df <- num_df[,all.analytes]"},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"generate-correlations","dir":"Articles","previous_headings":"WGCNA, Integration of multi-omics","what":"Generate Correlations","title":"standardcor-WGCNA","text":"use implementation sparse spearman correlation saves memory time. helps prevent notebook crashing! use Spearman rank correlationsince affected scalar multiples log-transformation. information technique can found : https://github.com/saketkc/blog/blob/main/2022-03-10/SparseSpearmanCorrelation2.ipynbs","code":"s.prots <- as(all_df[,cat.prots], \"sparseMatrix\") s.primary <- as(all_df[,cat.primary], \"sparseMatrix\") s.lipid <- as(all_df[,cat.lipid], \"sparseMatrix\") s.amines <- as(all_df[,cat.amines], \"sparseMatrix\") # Within-category correlations Z.pp <- SparseSpearmanCor2(s.prots) Z.mm <- SparseSpearmanCor2(s.primary) Z.ll <- SparseSpearmanCor2(s.lipid) Z.aa <- SparseSpearmanCor2(s.amines) # Cross-category correlations Z.pm <- SparseSpearmanCor2(s.prots, s.primary) Z.pl <- SparseSpearmanCor2(s.prots, s.lipid) Z.pa <- SparseSpearmanCor2(s.prots, s.amines) Z.ml <- SparseSpearmanCor2(s.primary, s.lipid) Z.ma <- SparseSpearmanCor2(s.primary, s.amines) Z.la <- SparseSpearmanCor2(s.lipid, s.amines) # Add row and column names to each dataframe dimnames(Z.pp) <- list(colnames(all_df[,cat.prots]), colnames(all_df[,cat.prots])) dimnames(Z.mm) <- list(colnames(all_df[,cat.primary]), colnames(all_df[,cat.primary])) dimnames(Z.ll) <- list(colnames(all_df[,cat.lipid]), colnames(all_df[,cat.lipid])) dimnames(Z.aa) <- list(colnames(all_df[,cat.amines]), colnames(all_df[,cat.amines]))  dimnames(Z.pm) <- list(colnames(all_df[,cat.prots]), colnames(all_df[,cat.primary])) dimnames(Z.pl) <- list(colnames(all_df[,cat.prots]), colnames(all_df[,cat.lipid])) dimnames(Z.pa) <- list(colnames(all_df[,cat.prots]), colnames(all_df[,cat.amines])) dimnames(Z.ml) <- list(colnames(all_df[,cat.primary]), colnames(all_df[,cat.lipid])) dimnames(Z.ma) <- list(colnames(all_df[,cat.primary]), colnames(all_df[,cat.amines])) dimnames(Z.la) <- list(colnames(all_df[,cat.lipid]), colnames(all_df[,cat.amines]))"},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"within-category-standarized-distributions","dir":"Articles","previous_headings":"WGCNA, Integration of multi-omics > Generate Correlations","what":"Within-category standarized distributions","title":"standardcor-WGCNA","text":"use standardcor functions estimate Beta parameters mean std dev omic.  blue line shows model distrubtion. can see fits background distribution. repeat process omics","code":"### Protein Protein Z.unique <- Z.pp[row(Z.pp) < col(Z.pp)] vw <- estimateShape(Z.pp) v.pp <- vw[1] w.pp <- vw[2] print(paste(\"Protein pairs: rho_ij ~ Beta(v =\",round(v.pp,3),\",w =\",round(w.pp,3),\")\")) #> [1] \"Protein pairs: rho_ij ~ Beta(v = 36.482 ,w = 34.418 )\"  fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,4),      main=\"Pairwise protein correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, v.pp, w.pp)/2, lwd=3, col=\"MediumBlue\") ### Metabolite-Metabolite Z.unique <- as.vector(Z.mm[row(Z.mm) < col(Z.mm)]) vw <- estimateShape(Z.mm) v.mm <- vw[1] w.mm <- vw[2] print(paste(\"Metabolite Pairs: rho_ij ~ Beta(v =\",round(v.mm,3),\",w =\",round(w.mm,3),\")\")) #> [1] \"Metabolite Pairs: rho_ij ~ Beta(v = 102.351 ,w = 101.913 )\"  fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise primary metabolite correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, v.mm, w.mm)/2, lwd=3, col=\"MediumBlue\") ### Lipid - Lipid Z.unique <- as.vector(Z.ll[row(Z.ll) < col(Z.ll)]) vw <- estimateShape(Z.ll) v.ll <- vw[1] w.ll <- vw[2] print(paste(\"Lipid Pairs: rho_ij ~ Beta(v =\",round(v.ll,3),\",w =\",round(w.ll,3),\")\")) #> [1] \"Lipid Pairs: rho_ij ~ Beta(v = 28.808 ,w = 25.44 )\"  fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise lipid correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, v.ll, w.ll)/2, lwd=3, col=\"MediumBlue\") ### Amine - Amine Z.unique <- as.vector(Z.aa[row(Z.aa) < col(Z.aa)]) vw <- estimateShape(Z.aa) v.aa <- vw[1] w.aa <- vw[2] print(paste(\"Amine Pairs: rho_ij ~ Beta(v =\",round(vw[1],3),\",w =\",round(vw[2],3),\")\")) #> [1] \"Amine Pairs: rho_ij ~ Beta(v = 116.617 ,w = 110.49 )\"  fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise amine correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, vw[1], vw[2])/2, lwd=3, col=\"MediumBlue\")"},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"cross-category","dir":"Articles","previous_headings":"","what":"Cross-category","title":"standardcor-WGCNA","text":"continue process cross-category omics combinations. consider.","code":"dim(Z.pm) #> [1] 283 149 Z.unique <- as.vector(Z.pm) # there are no self-comparisons, nor are there repeats due to symmetry vw <- estimateShape(Z.pm) v.pm <- vw[1] w.pm <- vw[2] print(paste(\"Protein-metabolite: rho_ij ~ Beta(v =\",round(vw[1],3),\",w =\",round(vw[2],3),\")\")) #> [1] \"Protein-metabolite: rho_ij ~ Beta(v = 177.718 ,w = 174.852 )\"  # The distribution of these cross-correlations is # markedly narrower than either of the contributing 'omics fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise Protein-metabolite correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, vw[1], vw[2])/2, lwd=3, col=\"MediumBlue\") # Modeling cross-category correlations: protein-lipid Z.unique <- as.vector(Z.pl) # there are no self-comparisons, nor are there repeats due to symmetry vw <- estimateShape(Z.pl) v.pl <- vw[1] w.pl <- vw[2] print(paste(\"Protein-metabolite: rho_ij ~ Beta(v =\",round(vw[1],3),\",w =\",round(vw[2],3),\")\")) #> [1] \"Protein-metabolite: rho_ij ~ Beta(v = 89.235 ,w = 89.5 )\"  # The distribution of these cross-correlations is # markedly narrower than either of the contributing 'omics fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise Protein-metabolite correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, vw[1], vw[2])/2, lwd=3, col=\"MediumBlue\") # Modeling cross-category correlations: protein-amine Z.unique <- as.vector(Z.pa) # there are no self-comparisons, nor are there repeats due to symmetry vw <- estimateShape(Z.pa) v.pa <- vw[1] w.pa <- vw[2] print(paste(\"Protein-metabolite: rho_ij ~ Beta(v =\",round(vw[1],3),\",w =\",round(vw[2],3),\")\")) #> [1] \"Protein-metabolite: rho_ij ~ Beta(v = 138.697 ,w = 137.27 )\"  # The distribution of these cross-correlations is # markedly narrower than either of the contributing 'omics fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise Protein-metabolite correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, vw[1], vw[2])/2, lwd=3, col=\"MediumBlue\") # Modeling cross-category correlations: primary-lipid Z.unique <- as.vector(Z.ml) # there are no self-comparisons, nor are there repeats due to symmetry vw <- estimateShape(Z.ml) v.ml <- vw[1] w.ml <- vw[2] print(paste(\"Protein-lipid: rho_ij ~ Beta(v =\",round(vw[1],3),\",w =\",round(vw[2],3),\")\")) #> [1] \"Protein-lipid: rho_ij ~ Beta(v = 189.953 ,w = 186.133 )\"  # The distribution of these cross-correlations is # markedly narrower than either of the contributing 'omics fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise Protein-lipid correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, vw[1], vw[2])/2, lwd=3, col=\"MediumBlue\") # Modeling cross-category correlations: primary-amine Z.unique <- as.vector(Z.ma) # there are no self-comparisons, nor are there repeats due to symmetry vw <- estimateShape(Z.ma) v.ma <- vw[1] w.ma <- vw[2] print(paste(\"Protein-amine: rho_ij ~ Beta(v =\",round(vw[1],3),\",w =\",round(vw[2],3),\")\")) #> [1] \"Protein-amine: rho_ij ~ Beta(v = 190.21 ,w = 186.059 )\"  # The distribution of these cross-correlations is # markedly narrower than either of the contributing 'omics fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise Protein-amine correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, vw[1], vw[2])/2, lwd=3, col=\"MediumBlue\") # Modeling cross-category correlations: lipid-amine Z.unique <- as.vector(Z.la) # there are no self-comparisons, nor are there repeats due to symmetry vw <- estimateShape(Z.la) v.la <- vw[1] w.la <- vw[2] print(paste(\"Lipid-amine: rho_ij ~ Beta(v =\",round(vw[1],3),\",w =\",round(vw[2],3),\")\")) #> [1] \"Lipid-amine: rho_ij ~ Beta(v = 131.107 ,w = 129.04 )\"  # The distribution of these cross-correlations is # markedly narrower than either of the contributing 'omics fine <- 40 Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Z.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,7.5),      main=\"Pairwise lipid-amine correlations\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, vw[1], vw[2])/2, lwd=3, col=\"MediumBlue\")"},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"merging-correlations-from-disparate-data-subsets","dir":"Articles","previous_headings":"Cross-category","what":"Merging correlations from disparate data subsets","title":"standardcor-WGCNA","text":"centering process similar , â€œquantile normalizationâ€. quantile normalization, value xix_i observed data cumulative probability pi=Pr{x<xi}p_i = \\Pr{\\{x < x_i\\} } among observed data {x}\\{ x \\} , accounting possibility ties finite dataset, average rank observed values equal xix_i divided total number observed values. value xix_i transformed value qiq_i cumulative probability pip_i according normalizing probability distribution DD (.e.Â Pr{d<qi|D}=pi\\Pr{ \\{ d < q_i | D \\} } = p_i). centering process, rather using rank xix_i observed data, use null model observed data; pip_i therefore p-value xix_i null model, transformed value qiq_i significance normalized probability distribution DD xix_i original null distribution. allows us merge datasets retaining significance original cohort. empirical distribution used null model two processes . , use beta distribution null model precisely appropriate null model correlations arbitrary independent vectors , reasonable assumptions, indistinguishable Beta distribution. believe appropriate null model correlations, suggest primary effect null model account effective number dimensions observed data, prior using 2D geometric model compute correlations observations.  now standardized correlations. mean variance distribution suggest model (shown blue) fits less well standardizing model (orange); consequence differences models fitted empirical distributions, indicates enrichment high correlations observed individual â€™omics distributions preserved. used quantile normalization, overabundance high correlations shifted lower correlation values, fitted blue model identical standardizing model.","code":"nu.std <- 34 # As wide as the widest compoonent, and centered at 0  Zc.pp <- centerBeta(Z.pp, v.pp, w.pp, nu.std) Zc.mm <- centerBeta(Z.mm, v.mm, w.mm, nu.std) Zc.ll <- centerBeta(Z.ll, v.ll, w.ll, nu.std) Zc.aa <- centerBeta(Z.aa, v.aa, w.aa, nu.std) Zc.pm <- centerBeta(Z.pm, v.pm, w.pm, nu.std) Zc.pl <- centerBeta(Z.pl, v.pl, w.pl, nu.std) Zc.pa <- centerBeta(Z.pa, v.pa, w.pa, nu.std) Zc.ml <- centerBeta(Z.ml, v.ml, w.ml, nu.std) Zc.ma <- centerBeta(Z.ma, v.ma, w.ma, nu.std) Zc.la <- centerBeta(Z.la, v.la, w.la, nu.std) # Combined, centered correlations  Zc <- matrix(0, nrow = length(all.analytes),              ncol = length(all.analytes)) rownames(Zc) <- all.analytes colnames(Zc) <- all.analytes ### Construct a final dataframe that contains all the correlation values. ### # Block-structured correlation matrix # Zc = [ PP     PM   PL  PA  | #      | PM^T   MM   ML   MA | #      | PL^T  MC^T  LL  LA  | #      | PA^T  MA^T  LA^T AA ] ### Zc[cat.prots, cat.prots] <- Zc.pp Zc[cat.primary,  cat.primary]  <- Zc.mm Zc[cat.lipid,  cat.lipid]  <- Zc.ll Zc[cat.amines,  cat.amines]  <- Zc.aa  Zc[cat.prots, cat.primary]  <- Zc.pm Zc[cat.primary, cat.prots]  <- t(Zc.pm)  Zc[cat.prots, cat.lipid]  <- Zc.pl Zc[cat.lipid, cat.prots]  <- t(Zc.pl)  Zc[cat.prots, cat.amines]  <- Zc.pa Zc[cat.amines, cat.prots]  <- t(Zc.pa)  Zc[cat.primary, cat.lipid]  <- Zc.ml Zc[cat.lipid, cat.primary]  <- t(Zc.ml)  Zc[cat.primary, cat.amines]  <- Zc.ma Zc[cat.amines, cat.primary]  <- t(Zc.ma)  Zc[cat.lipid, cat.amines]  <- Zc.la Zc[cat.amines, cat.lipid]  <- t(Zc.la)   print(str_c(\"nrow: \", nrow(Zc))) #> [1] \"nrow: 1465\" Zc[1:5,1:5] #>              1433Z        A1AG2        A1AT        A1BG       A2AP #> 1433Z  1.000000000 -0.009608999  0.01402562 -0.02168788 -0.3082587 #> A1AG2 -0.009608999  1.000000000  0.11051047  0.07297294 -0.1927106 #> A1AT   0.014025625  0.110510466  1.00000000  0.08730883 -0.3104823 #> A1BG  -0.021687883  0.072972937  0.08730883  1.00000000 -0.0553188 #> A2AP  -0.308258749 -0.192710596 -0.31048227 -0.05531880  1.0000000 Z.unique <- Zc[row(Zc) < col(Zc)] print(paste(\"Target: rho_ij ~ Beta(v =\",round(nu.std,3),\",w =\",round(nu.std,3),\")\")) #> [1] \"Target: rho_ij ~ Beta(v = 34 ,w = 34 )\"  x <- (1+Z.unique)/2 mZ <- mean(x) s2Z <- var(x) v.c <- mZ*(mZ*(1-mZ)/s2Z - 1) w.c <- (1-mZ)*(mZ*(1-mZ)/s2Z - 1) print(paste(\"Method of moments: rho_ij ~ Beta(v =\",round(v.c,3),\",w =\",round(w.c,3),\")\")) #> [1] \"Method of moments: rho_ij ~ Beta(v = 21.514 ,w = 21.308 )\"   fine <- 100 Zc.unique <- as.vector(Zc[row(Zc) < col(Zc)]) Bs <- (c(-fine:(1+fine))-0.5)/fine hist(Zc.unique, breaks=Bs, xlab=\"Correlation\", ylab=\"Density\", ylim=c(0,5),      main=\"All pairwise correlations, centered\", prob=TRUE) box() abline(v=c(-1:1),lty=3)  r <- c(-fine:fine)/fine lines(r, dbeta((1+r)/2, nu.std, nu.std)/2, lwd=3, col=\"orangered\") lines(r, dbeta((1+r)/2, v.c, w.c)/2, lwd=3, col=\"MediumBlue\")"},{"path":"https://longevity-consortium.github.io/standardcor/articles/standardcor-WGCNA.html","id":"wgcna","dir":"Articles","previous_headings":"","what":"WGCNA","title":"standardcor-WGCNA","text":"code follows standard WGCNA analysis. information can found : https://peterlangfelder.com/2018/11/25/wgcna-resources---web/       Now identified modules asssociated phenotype, can take analysis many directions. Please refer WGCNA documentation information.","code":"#Manually convert the pairwise correlation DF to the signed network DF Zc_signed <- 0.5 + 0.5 * Zc  print(str_c(\"nrow: \", nrow(Zc_signed))) #> [1] \"nrow: 1465\" #Choose a set of soft-thresholding powers powers <- c(c(1:10), seq(from=11, to=15, by=1)) cutoff <- 0.8  #Call the network topology analysis function sft <- pickSoftThreshold.fromSimilarity(Zc_signed, RsquaredCut=cutoff, powerVector=powers, blockSize=5000, verbose=5) #>  pickSoftThreshold: calculating connectivity for given powers... #>    ..working on genes 1 through 1465 of 1465 #> Warning: executing %dopar% sequentially: no parallel backend registered #>    Power SFT.R.sq  slope truncated.R.sq mean.k. median.k. max.k. #> 1      1  0.13800 10.800          0.949  736.00   735.000  797.0 #> 2      2  0.13000  4.190          0.917  378.00   375.000  450.0 #> 3      3  0.00102 -0.188          0.731  199.00   195.000  270.0 #> 4      4  0.34300 -2.250          0.722  107.00   103.000  176.0 #> 5      5  0.77800 -2.830          0.859   59.80    55.300  124.0 #> 6      6  0.91600 -2.820          0.919   34.50    30.400   92.5 #> 7      7  0.96200 -2.510          0.952   20.70    17.000   73.1 #> 8      8  0.93900 -2.260          0.922   13.00     9.640   60.6 #> 9      9  0.96400 -2.000          0.954    8.64     5.620   52.0 #> 10    10  0.96300 -1.810          0.953    6.06     3.300   46.0 #> 11    11  0.96000 -1.670          0.949    4.49     2.040   41.6 #> 12    12  0.97100 -1.550          0.963    3.52     1.290   38.4 #> 13    13  0.97600 -1.460          0.970    2.89     0.808   35.8 #> 14    14  0.97800 -1.400          0.975    2.47     0.526   33.9 #> 15    15  0.96900 -1.360          0.965    2.18     0.339   32.3  #Plot the results options(repr.plot.width=9, repr.plot.height=5) par(mfrow=c(1,2)) cex1 <- 0.8 ##Scale-free topology fit index as a function of the soft-thresholding power plot(sft$fitIndices[,1], -sign(sft$fitIndices[,3])*sft$fitIndices[,2],      xlab=\"Soft Threshold (power)\", ylab=\"Scale Free Topology Model Fit, signed R^2\", type=\"n\",      main=paste(\"Scale independence\")) text(sft$fitIndices[,1], -sign(sft$fitIndices[,3])*sft$fitIndices[,2],      labels=powers, cex=cex1, col=\"black\") ##Line corresponds to using an R^2 cut-off of h abline(h=cutoff, col=\"red\") ##Mean connectivity as a function of the soft-thresholding power plot(sft$fitIndices[,1], sft$fitIndices[,5],      xlab=\"Soft Threshold (power)\", ylab=\"Mean Connectivity\", type=\"n\",      main=paste(\"Mean connectivity\")) text(sft$fitIndices[,1], sft$fitIndices[,5], labels=powers, cex=cex1, col=\"black\") print(str_c(\"Estimated soft-thresholding power: \", sft$powerEstimate)) #> [1] \"Estimated soft-thresholding power: 6\" #Choose the power that best approximates a scale free topology while still maintaining high level of connectivity in the network softPower <- sft$powerEstimate softPower #> [1] 6 #Generate the adjacency matrix using the chosen soft-thresholding power adjacency <- adjacency.fromSimilarity(Zc, power=softPower, type=\"signed\")  print(str_c(\"nrow: \", nrow(adjacency))) #> [1] \"nrow: 1465\" #head(adjacency)  #Turn adjacency into topological overlap ##You can input whatever matrix you want here! # Turn adjacency into topological overlap TOM = TOMsimilarity(adjacency,TOMType = \"signed\"); #> ..connectivity.. #> ..matrix multiplication (system BLAS).. #> ..normalization.. #> ..done. # Turn into distance matrix dissTOM = 1-TOM colnames(dissTOM) <- colnames(all_df) rownames(dissTOM) <- colnames(dissTOM) # Cluster the TOM distance matrix to find modules # Can call whatever clusting method you want here  # Call the hierarchical clustering function geneTree = hclust(as.dist(dissTOM), method = \"ward.D2\"); # Plot the resulting clustering tree (dendrogram) #sizeGrWindow(12,9) plot(geneTree, xlab=\"\", sub=\"\", main = \"Gene clustering on TOM-based dissimilarity\",     labels = FALSE, hang = 0.04); box() #Larger modules can be easier to interpret, so we set the minimum module size relatively high minModuleSize <- max(c(20, round(ncol(all_df)/200, digits=0))) print(str_c(\"minClusterSize = \", minModuleSize)) #> [1] \"minClusterSize = 20\"  #Module identification using dynamic tree cut dynamicMods <- cutreeDynamic(dendro=geneTree, distM=dissTOM,                              deepSplit=4, pamStage=TRUE, pamRespectsDendro=FALSE,                              minClusterSize=minModuleSize) #>  ..cutHeight not given, setting it to 5.17  ===>  99% of the (truncated) height range in dendro. #>  ..done. table(dynamicMods) #> dynamicMods #>   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  #> 431 183 105  79  60  56  53  40  40  38  36  36  34  32  31  30  30  28  28  27  #>  21  22  23  #>  25  22  21  #Convert numeric lables into colors dynamicColors <- labels2colors(dynamicMods) table(dynamicColors) #> dynamicColors #>         black          blue         brown          cyan     darkgreen  #>            53           183           105            32            22  #>       darkred darkturquoise         green   greenyellow        grey60  #>            25            21            60            36            30  #>     lightcyan    lightgreen   lightyellow       magenta  midnightblue  #>            30            28            28            40            31  #>          pink        purple           red     royalblue        salmon  #>            40            38            56            27            34  #>           tan     turquoise        yellow  #>            36           431            79  #Plot the dendrogram and colors underneath options(repr.plot.width=12, repr.plot.height=6) plotDendroAndColors(geneTree, dynamicColors, \"Dynamic Tree Cut\",                     dendroLabels=FALSE, hang=0.03,                     addGuide=TRUE, guideHang=0.05,                     main=\"Gene dendrogram and module colors\") #Calculate eigengenes MEList <- moduleEigengenes(all_df, colors=dynamicColors, impute=TRUE, nPC=2) MEs <- MEList$eigengenes print(str_c(\"nrow: \", nrow(MEs))) #> [1] \"nrow: 500\" head(MEs) #>       MEblack       MEblue      MEbrown     MEcyan  MEdarkgreen    MEdarkred #> 1  0.01769292 -0.051160929 -0.044378898 0.01880385  0.026300750  0.056728907 #> 2  0.04529345 -0.027186301  0.081716559 0.04014007 -0.065170342 -0.027008752 #> 3  0.03116145 -0.064842191  0.050969124 0.03833421 -0.027185755  0.034050318 #> 4 -0.01276477 -0.011334389 -0.024473474 0.01594035  0.005975657  0.003775484 #> 5  0.02255580 -0.108910882  0.020253466 0.01890705 -0.087284917 -0.091232433 #> 6  0.01620370 -0.008809908  0.006610517 0.04426588  0.015338840 -0.008089793 #>   MEdarkturquoise      MEgreen MEgreenyellow     MEgrey60  MElightcyan #> 1    -0.054838015  0.007742242  -0.006937975  0.023279528 -0.025568891 #> 2     0.081574426  0.027353168   0.037621498  0.003638059  0.031146586 #> 3    -0.024189996  0.075929834  -0.010496068  0.028760979  0.011325765 #> 4     0.001072615  0.026527175  -0.040967189  0.020079862 -0.003064056 #> 5     0.028547619 -0.042833872  -0.050583886 -0.053727660  0.044451861 #> 6     0.067679731  0.052427209   0.029833876 -0.008533758  0.029449067 #>    MElightgreen MElightyellow    MEmagenta MEmidnightblue      MEpink #> 1  0.0831709906   0.054407199 -0.011885640    -0.02318667 -0.05903108 #> 2 -0.0223851354   0.054747059  0.002597031    -0.01508430  0.04217619 #> 3  0.0063840596   0.041592726 -0.014797012    -0.04415341 -0.01475084 #> 4 -0.0280092308   0.008061438  0.010143341     0.00139921 -0.01220036 #> 5  0.0120321816  -0.018502448 -0.124222277    -0.09145141  0.06847406 #> 6 -0.0004212465  -0.021378014 -0.060088746    -0.07637588  0.05340293 #>       MEpurple        MEred  MEroyalblue     MEsalmon        MEtan MEturquoise #> 1  0.001787618 -0.044040999  0.051904178  0.001376754 -0.045764356 0.095295608 #> 2  0.066292815 -0.015444106  0.009153908  0.035193925  0.009062544 0.002543363 #> 3  0.046233328 -0.042038230  0.029811076  0.013927902 -0.055651008 0.010488607 #> 4 -0.019138426 -0.002514375  0.009501255  0.040701101  0.010042727 0.016468088 #> 5 -0.006243943 -0.121599175  0.012112474 -0.013905834 -0.100395931 0.064945152 #> 6 -0.056277156 -0.066816892 -0.019450368  0.024405079 -0.042246700 0.052961943 #>       MEyellow #> 1  0.021543695 #> 2  0.048948253 #> 3  0.056389149 #> 4 -0.024116585 #> 5 -0.023709852 #> 6  0.005627724  #Calculate dissimilarity of module eigengenes MEDiss <- 1 - cor(MEs, use=\"pairwise.complete.obs\")  #Cluster module eigengenes METree <- hclust(as.dist(MEDiss), method=\"ward.D2\")  #Plot the result options(repr.plot.width=10, repr.plot.height=5) plot(METree, main=\"Clustering of module eigengenes\",      xlab=\"\", sub=\"\") MEDissThres <- 0.3 abline(h=MEDissThres, col=\"red\") #Call an automatic merging function merge <- mergeCloseModules(all_df, dynamicColors, cutHeight=MEDissThres, verbose=0)  #Eigengenes of the new merged modules mergedMEs <- merge$newMEs  #The merged module colors mergedColors <- merge$colors table(mergedColors) #> mergedColors #>          blue         brown          cyan     darkgreen darkturquoise  #>           219           283            32            47            21  #>         green   greenyellow        grey60   lightyellow       magenta  #>            60            36            30            28            40  #>  midnightblue        purple           red        salmon     turquoise  #>            31            38            56            34           431  #>        yellow  #>            79  #Plot the dendrogram and module colors options(repr.plot.width=12, repr.plot.height=6) plotDendroAndColors(geneTree, cbind(dynamicColors, mergedColors),                     c(\"Dynamic Tree Cut\", \"Merged dynamic\"),                     dendroLabels=FALSE, hang=0.03,                     addGuide=TRUE, guideHang=0.05,                     main=\"Gene dendrogram and module colors\") #Rename moduleColors <- mergedColors MEs <- mergedMEs #Rename moduleColors <- mergedColors MEs <- mergedMEs  #Clean the module eigengene table eigengene_df <- MEs %>%     rownames_to_column(var=\"public_client_id\") names(eigengene_df)[2:ncol(eigengene_df)] <- names(eigengene_df)[2:ncol(eigengene_df)] %>%     str_replace(., \"^ME\", \"\") %>%     str_to_title(.) print(\"Module eigengene table\") #> [1] \"Module eigengene table\" print(str_c(\"- nrow: \", nrow(eigengene_df))) #> [1] \"- nrow: 500\" head(eigengene_df) #>   public_client_id   Turquoise         Blue          Red      Magenta #> 1                1 0.095295608 -0.052219663 -0.044040999 -0.011885640 #> 2                2 0.002543363 -0.001818211 -0.015444106  0.002597031 #> 3                3 0.010488607 -0.063797679 -0.042038230 -0.014797012 #> 4                4 0.016468088  0.005857586 -0.002514375  0.010143341 #> 5                5 0.064945152 -0.107111127 -0.121599175 -0.124222277 #> 6                6 0.052961943 -0.040473960 -0.066816892 -0.060088746 #>   Midnightblue        Brown Darkturquoise  Lightyellow       Cyan        Green #> 1  -0.02318667  0.003551357  -0.054838015  0.054407199 0.01880385  0.007742242 #> 2  -0.01508430  0.035070077   0.081574426  0.054747059 0.04014007  0.027353168 #> 3  -0.04415341  0.020193625  -0.024189996  0.041592726 0.03833421  0.075929834 #> 4   0.00139921 -0.011460655   0.001072615  0.008061438 0.01594035  0.026527175 #> 5  -0.09145141  0.035817706   0.028547619 -0.018502448 0.01890705 -0.042833872 #> 6  -0.07637588  0.018605726   0.067679731 -0.021378014 0.04426588  0.052427209 #>         Grey60    Darkgreen  Greenyellow       Salmon       Purple       Yellow #> 1  0.023279528  0.046261983 -0.006937975  0.001376754  0.001787618  0.021543695 #> 2  0.003638059 -0.046143879  0.037621498  0.035193925  0.066292815  0.048948253 #> 3  0.028760979  0.007247343 -0.010496068  0.013927902  0.046233328  0.056389149 #> 4  0.020079862  0.006183944 -0.040967189  0.040701101 -0.019138426 -0.024116585 #> 5 -0.053727660 -0.095823745 -0.050583886 -0.013905834 -0.006243943 -0.023709852 #> 6 -0.008533758  0.003092429  0.029833876  0.024405079 -0.056277156  0.005627724 ##Sample metadata sample_tbl <- pheno[pheno$subjectID %in% rownames(MEs),] print(\"Sample metadata after the filter\") #> [1] \"Sample metadata after the filter\" print(str_c(\"- nrow: \", nrow(sample_tbl))) #> [1] \"- nrow: 500\"  #Code sex and race phenotype_tbl <- sample_tbl   phenotype_tbl <- phenotype_tbl[match(rownames(MEs), rownames(phenotype_tbl)),] #Calculate the numbers of modules and samples #nModules <- ncol(MEs) nSamples <- nrow(phenotype_tbl)  #Names (colors) of the modules modNames = substring(names(MEs), 3)  ##Check ID order before the cor() function print(str_c(\"Matched IDs?: \", all(rownames(MEs)==rownames(phenotype_tbl)))) #> [1] \"Matched IDs?: TRUE\"  #Calculate moduleâ€“trait relationship moduleTraitCor <- as.data.frame(cor(MEs, phenotype_tbl, use=\"p\")) rownames(moduleTraitCor) <- str_to_title(modNames) print(\"Moduleâ€“trait relationship table\") #> [1] \"Moduleâ€“trait relationship table\" print(str_c(\"nrow: \", nrow(moduleTraitCor))) #> [1] \"nrow: 16\"  #Calculate statisitcal significance of moduleâ€“trait relationship MTRpval <- as.data.frame(corPvalueStudent(as.matrix(moduleTraitCor), nSamples)) rownames(MTRpval) <- str_to_title(modNames) print(\"Moduleâ€“trait relationship p-value table\") #> [1] \"Moduleâ€“trait relationship p-value table\" print(str_c(\"- nrow: \", nrow(MTRpval))) #> [1] \"- nrow: 16\"  #Eliminate the dummy module (Grey) moduleTraitCor <- moduleTraitCor[rownames(moduleTraitCor)!=\"Grey\",] MTRpval <- MTRpval[rownames(MTRpval)!=\"Grey\",]  #P-value adjustment across modules (per trait) using Benjaminiâ€“Hochberg method MTRpval_adj <- as.data.frame(apply(MTRpval, 2, function(x){p.adjust(x, length(x), method=\"BH\")})) print(\"Moduleâ€“trait relationship adjusted p-value table\") #> [1] \"Moduleâ€“trait relationship adjusted p-value table\" print(str_c(\"- nrow: \", nrow(MTRpval_adj))) #> [1] \"- nrow: 16\"  #Prepare text labels as matrix textMatrix <- paste(\"r = \",signif(as.matrix(moduleTraitCor), 3),\"\\n(P = \",                     signif(as.matrix(MTRpval_adj), 2),\")\", sep=\"\") dim(textMatrix) <- dim(moduleTraitCor) #Revert module names back to apply color conversion temp_c <- rownames(moduleTraitCor) %>%     str_to_lower(.) %>%     str_c(\"ME\",.) #Visualize options(repr.plot.width=10, repr.plot.height=10) par(mar=c(5, 5, 3, 2)) labeledHeatmap(Matrix=moduleTraitCor,                xLabels=colnames(moduleTraitCor),                yLabels=temp_c,                #ySymbols=rownames(moduleTraitCor),                colorLabels=FALSE,                colors=blueWhiteRed(50),                textMatrix=textMatrix,                setStdMargins=FALSE,                cex.text=1,                zlim=c(-1,1),                main=paste(\"Moduleâ€“trait relationships\"))"},{"path":"https://longevity-consortium.github.io/standardcor/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Max Robinson. Maintainer.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Robinson M, Pflieger L (2026). standardcor: Empirical Null Models Pairwise Correlationsâ€“Estimation Standardization. R package version 0.1.0, https://longevity-consortium.github.io/standardcor/.","code":"@Manual{,   title = {standardcor: Empirical Null Models of Pairwise Correlations--Estimation and Standardization},   author = {Max Robinson and Lance Pflieger},   year = {2026},   note = {R package version 0.1.0},   url = {https://longevity-consortium.github.io/standardcor/}, }"},{"path":"https://longevity-consortium.github.io/standardcor/index.html","id":"standardcor","dir":"","previous_headings":"","what":"Empirical Null Models of Pairwise Correlations--Estimation and Standardization","title":"Empirical Null Models of Pairwise Correlations--Estimation and Standardization","text":"package provides utilities interpreting correlation coefficients distances adjacency weights clustering. Pairwise correlations geometric interpretation cosine angle two vectors length N, important note regardless large N , angle inherently 2-dimensional concept. standard approaches convert pairwise correlations distances retain 2-dimensional behavior. pairs set vectors considered, general vectors similar planes different orientations, distribution across pairs reflect structure N-dimensional space. package therefore uses Beta distribution null model bulk, spurious correlations extend standard approaches, producing N-dimensional distances. null model also provides alternative, statistical approach defining adjacency weights first step constructing network model correlations Weighted Correlation Network Analysis. important contribution package, though, co-clustering datasets collected different high-throughput (â€™omics) technologies, distribution spurious correlations general different statistical characteristics. fitting distribution separately, package provides means standardize correlation values across different technologies, well cross-correlations data measured different technologies. standardization addresses differences variance spurious correlations (noise levels) across â€™omics platforms, reducing observed tendency correlations noisier platforms drive clustering correlations less noisy platforms, correlations across platforms. package therefore supports correlation-based clustering providing conceptual framework, methods fitting distribution spurious correlations, methods standardizing correlation values across â€™omics platforms, new methods converting correlations distances network adjacencies.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/index.html","id":"installation-in-r","dir":"","previous_headings":"","what":"Installation in R:","title":"Empirical Null Models of Pairwise Correlations--Estimation and Standardization","text":"","code":"if (!require(\"remotes\", quietly = TRUE))    install.packages(\"remotes\")  remotes::install_github(\"longevity-consortium/standardcor\")"},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparseSpearmanCor2.html","id":null,"dir":"Reference","previous_headings":"","what":"Sparse implementation of Spearman Correlation â€” SparseSpearmanCor2","title":"Sparse implementation of Spearman Correlation â€” SparseSpearmanCor2","text":"Computes Spearman correlations either among columns one sparse matrix (X) columns two sparse matrices (X, Y). implementation specific Spearman correlation highly efficient; efficiency results avoiding unnecessary computations involving ranks zero entries, well equivalence rank correlations ranks shifted constant. Copied without code changes, textual change comments, public blog repository     https://github.com/saketkc/blog/blob/main/2022-03-10/SparseSpearmanCorrelation2.ipynb owned Saket Choudhary Mumbai, India 23 Mar 2024","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparseSpearmanCor2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sparse implementation of Spearman Correlation â€” SparseSpearmanCor2","text":"","code":"SparseSpearmanCor2(X, Y = NULL, cov = FALSE)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparseSpearmanCor2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sparse implementation of Spearman Correlation â€” SparseSpearmanCor2","text":"X sparse data matrix Y second sparse data matrix. X provided, columns X correlated ; Y also specified, columns X cross-correlated columns Y. cov optional covariance matrix use computing correlations. Passed qlcMatrix::corSparse().","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparseSpearmanCor2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sparse implementation of Spearman Correlation â€” SparseSpearmanCor2","text":"matrix M Spearman correlations. .null(Y), M[,j] = cor(X[,],X[,j],method='s'), otherwise M[,j] = cor(X[,],Y[j],method='s'). implementation considerably faster others X Y sparse.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparsifiedRanks2.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace non-zero entries in a sparse matrix with non-zero ranks â€” SparsifiedRanks2","title":"Replace non-zero entries in a sparse matrix with non-zero ranks â€” SparsifiedRanks2","text":"Creates rank matrix sparse matrix X using following approach: 1. Use non-zero enries column calculate ranks 2. Add (z-1)/2 ranks (non-zero entries changed). z number zeros column Since entries shifted constant (zeros already shifted), covariance matrix shifted matrix rank matrix entire matrix (zeros also rank = (z+1)/2) z number zeros rank matrix can used calculate Spearman's correlation coefficient (via pearson correlation) Copied without code changes minor changes comments, public blog repository     https://github.com/saketkc/blog/blob/main/2022-03-10/SparseSpearmanCorrelation2.ipynb owned Saket Choudhary Mumbai, India 23 Mar 2024","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparsifiedRanks2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace non-zero entries in a sparse matrix with non-zero ranks â€” SparsifiedRanks2","text":"","code":"SparsifiedRanks2(X)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparsifiedRanks2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace non-zero entries in a sparse matrix with non-zero ranks â€” SparsifiedRanks2","text":"X sparse data matrix Y second sparse data matrix. X provided, columns X correlated ; Y also specified, columns X cross-correlated columns Y.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/SparsifiedRanks2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Replace non-zero entries in a sparse matrix with non-zero ranks â€” SparsifiedRanks2","text":"sparse matrix ranks non-zero entries","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/betaDistance.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Dimensionally-adjusted correlation distances â€” betaDistance","title":"Calculate Dimensionally-adjusted correlation distances â€” betaDistance","text":"standard (Euclidean) transformation correlation coefficients distances based 2-dimensional geometry plane formed correlated vectors origin. However, simulations random vectors demonstrate probability correlations random vectors reaching given correlation coefficient depends number dimensions spanned random vectors. function allows transforming correlation coefficients distances reflect null distribution, corresponds Beta distribution can estimated bulk correlations assumption majority correlations null model.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/betaDistance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Dimensionally-adjusted correlation distances â€” betaDistance","text":"","code":"betaDistance(r, v = 1, w = 1, mix = 1, unsigned = TRUE)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/betaDistance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Dimensionally-adjusted correlation distances â€” betaDistance","text":"r numeric object containing correlation coefficients v first shape parameter 2-parameter Beta distribution w second shape parameter 2-parameter Beta distribution mix relative weight squared Euclidean distance Beta distance penalty. mix = 0, Beta distance used correlation significantly beyond null distribution essentially zero distance. mix = 1, correlation corresponding zero distance +/-1, Beta distance acts penalty top squared Euclidean distance. intermediate values allowed, recommended. unsigned TRUE (default), computes distance association (|r|) instead correlation (r).","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/betaDistance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Dimensionally-adjusted correlation distances â€” betaDistance","text":"numeric object containing dimensionally-adjusted distances","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/centerBeta.html","id":null,"dir":"Reference","previous_headings":"","what":"Center correlations â€” centerBeta","title":"Center correlations â€” centerBeta","text":"null distribution model r ~ 2 Beta(v,w) - 1 centering (target) distribution r_centered ~ 2 Beta(nu, nu) - 1","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/centerBeta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Center correlations â€” centerBeta","text":"","code":"centerBeta(r, v, w, nu)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/centerBeta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Center correlations â€” centerBeta","text":"r correlation center v first parameter Beta distribution w second parameter Beta distribution nu parameter target Beta distribution","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/centerBeta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Center correlations â€” centerBeta","text":"centered correlation","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/estimateShape.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate parameters of a Beta distribution null model â€” estimateShape","title":"Estimate parameters of a Beta distribution null model â€” estimateShape","text":"function estimates shape parameters v w Beta(v,w) distribution fit bulk (background) distribution given set correlations. objective estimating process match density distribution mode breadth distribution around mode, goal estimating model bulk (null) distribution assumption correlation coefficents provided corSet contains modest number values drawn null distribution. Note possible provide infallible definition desired null model; method therefore necessarily heuristic provide satisfactory result cases. occurs, user encouraged use \"left\" \"right\" parameters needed provide satisfactory null model.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/estimateShape.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate parameters of a Beta distribution null model â€” estimateShape","text":"","code":"estimateShape(   corSet,   left = 0,   right = 0,   plot = FALSE,   fine = NULL,   trim = 0.01,   ... )"},{"path":"https://longevity-consortium.github.io/standardcor/reference/estimateShape.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate parameters of a Beta distribution null model â€” estimateShape","text":"corSet vector non-unique, non-self correlation coefficients. left adjustment estimated value w. parameter provided effect changing estimated v parameter can assessed plot. Note mean Beta(v,w) v/(v+w), adding w moves mean left. right adjustment estimated value v. Adding value v moves mean Beta distribution right; Adding v w decreases variance. plot TRUE, plot distribution shown, along fitted distribution Beta(v,w). ev ew estimated parameters, v = ev + right, w = ew + left. fine Half number bins histogram. fine = NULL (default), appropriate value 5 100 used. trim mean correlations estimated robustly making initial Beta model, trimming values quantile trim/2 quantile 1-trim/2 initial model (0 <= trim < 1). trimming reduces number correlations remain less five, trimming disabled full set correlations used.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/estimateShape.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate parameters of a Beta distribution null model â€” estimateShape","text":"vector c(v, w) containing two Beta parameters (see plot=TRUE ).","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/euclidDistance.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Euclidean distance â€” euclidDistance","title":"Calculate Euclidean distance â€” euclidDistance","text":"Calculates distance network correlation coefficient matrix using Euclidean function.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/euclidDistance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Euclidean distance â€” euclidDistance","text":"","code":"euclidDistance(mat, v = 1, w = 1, type = \"unsigned\")"},{"path":"https://longevity-consortium.github.io/standardcor/reference/euclidDistance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Euclidean distance â€” euclidDistance","text":"mat matrix correlation coefficients v first parameter beta distribution w second parameter beta distribution type type distance calculate","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/euclidDistance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Euclidean distance â€” euclidDistance","text":"matrix distances","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/interpolatedAdjacency.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a (standardized) correlation value r to an adjacency â€” interpolatedAdjacency","title":"Convert a (standardized) correlation value r to an adjacency â€” interpolatedAdjacency","text":"Interpolates adjacency value 0..1 correlation value -1 <= r <= 1 tabulated soft thresholding function.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/interpolatedAdjacency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a (standardized) correlation value r to an adjacency â€” interpolatedAdjacency","text":"","code":"interpolatedAdjacency(r, adjTable)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/interpolatedAdjacency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a (standardized) correlation value r to an adjacency â€” interpolatedAdjacency","text":"r numeric object containing (standardized) correlation coefficients adjTable tabulated soft-threshold adjacency function, produced nullModelAdjacencyTable()","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/interpolatedAdjacency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a (standardized) correlation value r to an adjacency â€” interpolatedAdjacency","text":"numeric object containing adjacencies correlation network model","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/multiOmicModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Spearman correlations and normalization distributions within and between multiple datasets â€” multiOmicModel","title":"Compute Spearman correlations and normalization distributions within and between multiple datasets â€” multiOmicModel","text":"Manages computation Spearman correlations within multiple, potentially large ('Omics) datasets, returning matrix correlations pairs analytes. listed dataset expected numeric matrix containing numeric data, however input list can contain either matrix (referred internal datasets) large datasets, name RDS file containing dataset (external datasets). approach handle unlimited amount data; least, internal datasets every pair external datasets must fit memory simultaneously. function computes null models, separated block-wise construction standardized correlation matrix analytes provide opportunity null model parameters computed adjusted needed prior use standardization.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/multiOmicModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Spearman correlations and normalization distributions within and between multiple datasets â€” multiOmicModel","text":"","code":"multiOmicModel(OmicsL, common = TRUE, min.samples = 5, annotate = FALSE, ...)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/multiOmicModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Spearman correlations and normalization distributions within and between multiple datasets â€” multiOmicModel","text":"OmicsL named list datasets. dataset may matrix (internal dataset) name RDS file containing matrix (external dataset). matrices interpreted rows representing samples columns representing analytes; analyte names must unique across datasets. common TRUE (default), samples common datasets used compute correlations. Otherwise, correlations computed across samples shared individual pair datasets. requiring correlations depend common set samples, setting common FALSE permits available data contribute cross-correlation; user decide appropriate. min.samples lower limit number samples analyte-analyte correlations may computed. parameter used ensure datasets overlap sufficiently samples provide least minimal ability compute correlations analytes. common set samples required across datasets, parameter minimum length common set samples; otherwise min.samples minimum number samples shared pair 'Omics datasets independently.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/multiOmicModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Spearman correlations and normalization distributions within and between multiple datasets â€” multiOmicModel","text":"list two parts: 'modelL', list Beta distributon null models raw correlation values every pair input ('Omics) datasets, 'analyteL', list analytes contributed 'Omics dataset. standardized correlation matrix can computed two lists using standardizeFromModel(), either directly adjustment individual null model parameters needed. rows columns final standardized correlation matrix labeled full set analytes listed analyteL, must unique; effort made render analyte names unique either functions.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/nullModelAdjacencyTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute soft threshold adjacency table for unsigned correlations from a symmetric Beta distribution null model â€” nullModelAdjacencyTable","title":"Compute soft threshold adjacency table for unsigned correlations from a symmetric Beta distribution null model â€” nullModelAdjacencyTable","text":"Tabulates soft thresholding function converting correlations directly adjacencies suitable network model set correlations among set analytes, example table transcriptomic, proteomic, metabolomic, 'Omics data. tabulated function uses background model random correlations expressed parameter nu: Prr | null model = Prx <= Beta(nu, nu) | x = (1+r)/2, estimate number correlations background model, estimate value r probability , given value misclassified, classified false negative. estimated number false negatives (FN) false positives (FP), probability FN / (FN + FP). serves appropriate soft-threshold function, probability 1/2 estimate number false negatives equals number false positives.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/nullModelAdjacencyTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute soft threshold adjacency table for unsigned correlations from a symmetric Beta distribution null model â€” nullModelAdjacencyTable","text":"","code":"nullModelAdjacencyTable(uniqueCor, v, scale = 2, bins = 100)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/nullModelAdjacencyTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute soft threshold adjacency table for unsigned correlations from a symmetric Beta distribution null model â€” nullModelAdjacencyTable","text":"uniqueCor numeric object containing full set non-self, unique correlation values consider. analyte-analyte correlation matrix Z, non-self, unique correlations can found using Z[row(Z) < col(Z)]. v parameter null model random correlations, Prr | null model = Prx <= Beta(v, v) | x = (1+r)/2. scale Center soft threshold relative estimated number correlations beyond fit null model. Defaults 2; higher values increase connectivity network model, lower values decrease .","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/nullModelAdjacencyTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute soft threshold adjacency table for unsigned correlations from a symmetric Beta distribution null model â€” nullModelAdjacencyTable","text":"data frame 2 columns, x y, tabulating y function x -1 <= x <= 1 0 <= y <= 1.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/powerDistance.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate power distance â€” powerDistance","title":"Calculate power distance â€” powerDistance","text":"Calculates distances WCNA correlation coefficient(s) using power function.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/powerDistance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate power distance â€” powerDistance","text":"","code":"powerDistance(r, k, unsigned = TRUE)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/powerDistance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate power distance â€” powerDistance","text":"r Correlation coefficient(s), -1 <= r <= 1. k power parameter. unsigned TRUE, function computes distances based unsigned associations (|r|), otherwise computes distances based signed correlations (r).","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/powerDistance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate power distance â€” powerDistance","text":"matrix distances","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/powerDistance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate power distance â€” powerDistance","text":"Correlation distance transformed follows: type = \"unsigned\"= 1 - abs(r)^k; type = \"signed\" = ((1 - r)/2)^k","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/sigmoidDistance.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate sigmoid distance â€” sigmoidDistance","title":"Calculate sigmoid distance â€” sigmoidDistance","text":"Calculates distances correlation coefficients using sigmoid function. WCNA converts correlations distances distances adjacencies construct adjacency matrix network model analyte relationships pairwise correlations; function used first step. distances can thought providing soft thresholding correlations. sigmoid function provides threshold parameter (tau0), specifies correlation value corresponding adjacency = 0.5; rate parameter (alpha > 0), controls \"hardness\" threshold.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/sigmoidDistance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate sigmoid distance â€” sigmoidDistance","text":"","code":"sigmoidDistance(r, alpha, tau0, unsigned = TRUE, stretch = FALSE)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/sigmoidDistance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate sigmoid distance â€” sigmoidDistance","text":"r Correlation coefficient(s) alpha alpha parameter sigmoid function (log slope tau0) tau0 tau0 parameter sigmoid function (correlation value corresponding distance = 0.5) stretch TRUE, distances rescaled range 0..1. (default: stretch=FALSE) type interpretation correlation coefficient signed (correlation) unsigned (association, default). value \"unsigned\" interpreted signed.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/sigmoidDistance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate sigmoid distance â€” sigmoidDistance","text":"distance(s) corresponding correlation coefficient(s)","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/standardcor-package.html","id":null,"dir":"Reference","previous_headings":"","what":"standardcor: Empirical Null Models of Pairwise Correlationsâ€“Estimation and Standardization â€” standardcor-package","title":"standardcor: Empirical Null Models of Pairwise Correlationsâ€“Estimation and Standardization â€” standardcor-package","text":"package addresses issues arising using correlation coefficients determine adjacencies network representations systems interacting variables, measured quantities gene transcripts, proteins, metabolites, microRNAs, lipids, related measurements collected high-throughput technologies employed molecular biological research. methods specific application, statistical intended practical value regardless field application; however, package motivated issues arising stated application.","code":""},{"path":[]},{"path":"https://longevity-consortium.github.io/standardcor/reference/standardizeFromModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Build a standardized correlation matrix using null models for each pair of component datasets â€” standardizeFromModel","title":"Build a standardized correlation matrix using null models for each pair of component datasets â€” standardizeFromModel","text":"Uses provided list null models (modelL) standardize type correlations target common null model specified v.std parameter, (r + 1)/2 ~ Beta(v.std, v.std), r standardized Spearman correlation coefficient two analytes modeled list 'Omics datasets. analtyes listed dataset components list parameter analtyeL, must unique contain analytes provided raw correlation matrix modelL. resulting standardized correlation matrix built blockwise full symmetric.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/standardizeFromModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build a standardized correlation matrix using null models for each pair of component datasets â€” standardizeFromModel","text":"","code":"standardizeFromModel(modelL, analyteL, v.std = 32)"},{"path":"https://longevity-consortium.github.io/standardcor/reference/standardizeFromModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build a standardized correlation matrix using null models for each pair of component datasets â€” standardizeFromModel","text":"modelL list--lists structure providing two inputs pair 'Omics datasets B: matrix modelL[[]][[B]][['cor']] raw Spearman correlation values, shape parameters modelL[[]][[B]][['shape']] == c(v,w) specifying null model (1+r_raw)/2 ~ Beta(v,w) raw correlations. v.std  analtyeL 'Omics dataset , analyteL[[]] lists analytes provided dataset . analyte identifiers required unique across datasets.","code":""},{"path":"https://longevity-consortium.github.io/standardcor/reference/standardizeFromModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build a standardized correlation matrix using null models for each pair of component datasets â€” standardizeFromModel","text":"symmetric matrix containing standardized Spearman correlation coefficients every pair analytes across datasets.","code":""}]
